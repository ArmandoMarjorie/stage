#include "BILSTM.hpp"
	
using namespace std;
using namespace dynet;


/**
 * \file BILSTM.cpp
*/


	/* Constructor */
	
	
/**
	* \brief BILSTM Constructor. Initializes a BILSTM.
	*
	* \param nblayer : number of layer.
	* \param inputdim : inputs dimension (embeddings dimension).
	* \param hiddendim : hidden states dimension.
	* \param dropout : dropout.
	* \param s : numero of the system used (3).
	* \param model : the model.
*/	
BiLSTM::BiLSTM(unsigned nblayer, unsigned inputdim, unsigned hiddendim, 
	float dropout, unsigned s, ParameterCollection& model, 
	bool original_lime) :
	RNN(nblayer, inputdim, hiddendim, dropout, s, model, original_lime)
{
	backward_lstm = new VanillaLSTMBuilder(nb_layers, input_dim, hidden_dim, model);
	p_W = model.add_parameters({NB_CLASSES, 4*hidden_dim});
}
	
	
	/* Predictions algorithms */
	

/**
	* \name predict
	* \brief Predicts the most probable label and gives the probability 
	* of each label for an instance. 
	* 
	* \param set : the data set. 
	* \param embedding : word embeddings.
	* \param num_sentence : numero of the instance.
	* \param cg : computation graph.
	* \param print_proba : true if we want the probability of each 
	* 	labels printed on screen, else false.
	* \param argmax : the predicted label.
	* 
	* \return The probability of each label for the "num_sentence"th instance.
*/	
vector<float> BiLSTM::predict(DataSet& set, Embeddings& embedding, unsigned num_sentence, ComputationGraph& cg, bool print_proba, unsigned& argmax)
{
	Expression x = run_KIM(set, embedding, num_sentence, cg);
	vector<float> probs = predict_algo(x, cg, print_proba, argmax); //in rnn.cpp
	return probs;
}


	/* Negative log softmax algorithms */


/**
	* \name get_neg_log_softmax
	* \brief Loss function (negative log likelihood).
	* 
	* \param set : the data set. 
	* \param embedding : word embeddings.
	* \param num_sentence : numero of the instance.
	* \param cg : computation graph.
	* 
	* \return The negative log likelihood of the "num_sentence"th 
	* instance's label after taking the softmax. 
*/
Expression BiLSTM::get_neg_log_softmax(DataSet& set, Embeddings& embedding, unsigned num_sentence, ComputationGraph& cg)
{
	Expression score = run_KIM(set, embedding, num_sentence, cg);
	Expression loss_expr = get_neg_log_softmax_algo(score, num_sentence, set); //in rnn.cpp
	return loss_expr;
}


	/* KIM-inspired system (system 3) */


/* Call this fonction twice (the first time for the premise, the second time for the hypothesis) 
 * sentence_repr[i] = representation of the i_th word by the KIM method
 */
/**
	* \name words_representation
	* \brief Gives a hidden representation for each word of the given sentence.
	* \details Use a Bi-LSTM to run a forward LSTM and backward LSTM on the sentence. 
	* The hidden states generated by the LSTMs at each time step are concatenated.
	* 
	* \param embedding : word embeddings.
	* \param set : the data set. 
	* \param is_premise : true if the premise is processed, else false.
	* \param cg : computation graph.
	* \param num_sentence : numero of the instance.
	* \param sentence_repr : matrix of size (number of words in the sentence, hidden dimension). 
	* sentence_repr[i] = representation of the i_th word
*/
void BiLSTM::words_representation(Embeddings& embedding, DataSet& set, bool is_premise,
	ComputationGraph& cg, unsigned num_sentence, vector<Expression>& sentence_repr)
{
	const unsigned nb_expr = set.get_nb_expr(is_premise, num_sentence);
	unsigned nb_words;
	if (apply_dropout)
	{ 
		forward_lstm->set_dropout(dropout_rate);
		backward_lstm->set_dropout(dropout_rate);
	}
	else 
	{
		forward_lstm->disable_dropout();
		backward_lstm->disable_dropout();
	}
	forward_lstm->new_graph(cg);  // reset LSTM builder for new graph
	forward_lstm->start_new_sequence(); //to do before add_input() and after new_graph()
	backward_lstm->new_graph(cg);  
	backward_lstm->start_new_sequence(); 

	vector<Expression> tmp;
	unsigned i;
	int j;
	unsigned wordID;
	
	// Run forward LSTM 
	for(i=0; i<nb_expr; ++i)
	{

		nb_words = set.get_nb_words(is_premise, num_sentence, i);
		for(unsigned k=0; k < nb_words; ++k)
		{
			wordID = set.get_word_id(is_premise, num_sentence, i, k);
			if(!original_LIME && wordID == 0) 
				continue;
			sentence_repr.push_back(forward_lstm->add_input( embedding.get_embedding_expr(cg, wordID) ) );
		}
	}
	
	// Run backward LSTM 
	for(j=nb_expr-1; j>=0; --j)
	{
		
		nb_words = set.get_nb_words(is_premise, num_sentence, static_cast<unsigned>(j));
		for(unsigned k=0; k < nb_words; ++k)
		{
			wordID = set.get_word_id(is_premise, num_sentence, static_cast<unsigned>(j), k);
			if(!original_LIME && wordID == 0) 
				continue;
			tmp.push_back(backward_lstm->add_input( 
					embedding.get_embedding_expr(cg, wordID) ) );
		}
	}
	
	// Concatenation 
	for(i=0; i<sentence_repr.size(); ++i)
	{
		vector<Expression> input_expr(2);
		input_expr[0] = sentence_repr[i];
		input_expr[1] = tmp[i];
		sentence_repr[i] = concatenate(input_expr);
	}

}


/**
	* \name create_attention_matrix
	* \brief The co-attention matrice.
	* 
	* \param cg : computation graph.
	* \param matrix : empty co-attention matrix that is initialized by the function.
	* \param premise_lstm_repr : vector containing the hidden representation of each premise's word.
	* \param hypothesis_lstm_repr : vector containing the hidden representation of each hypothesis' word.

*/
void BiLSTM::create_attention_matrix(vector< vector<float> >& matrix, 
	vector<Expression>& premise_lstm_repr, vector<Expression>& hypothesis_lstm_repr)
{
	const unsigned premise_size = premise_lstm_repr.size();
	const unsigned hypothesis_size = hypothesis_lstm_repr.size();
	for(unsigned i=0; i<premise_size; ++i)
	{
		for(unsigned j=0; j<hypothesis_size; ++j)
		{
			Expression e = transpose(premise_lstm_repr[i]) * hypothesis_lstm_repr[j];
			matrix[i][j] = as_scalar( e.value() );
		}
	}
}


/**
	* \name compute_alpha
	* \brief The normalized attention weight matrice (softmax on the premise).
	* 
	* \param cg : computation graph.
	* \param alpha_matrix : empty normalized attention weight matrice that is initialized by the function.
	* \param matrix : co-attention matrix.
*/
void BiLSTM::compute_alpha(vector< vector<float> >& alpha_matrix, 
	vector< vector<float> >& matrix)
{
	const unsigned premise_size = matrix.size();
	const unsigned hypothesis_size = matrix[0].size();
	float result=0;
	for(unsigned i=0; i<premise_size; ++i)
	{
		for(unsigned j=0; j<hypothesis_size; ++j)
		{
			result = 0;
			for(unsigned k=0; k<hypothesis_size; ++k)
					result += exp( matrix[i][k] );
			alpha_matrix[i][j] = exp( matrix[i][j] ) / result;
		}
	}
}


/**
	* \name compute_beta
	* \brief The normalized attention weight matrice (softmax on the hypothesis).
	* 
	* \param cg : computation graph.
	* \param beta_matrix : empty normalized attention weight matrice that is initialized by the function.
	* \param matrix : co-attention matrix.
*/
void BiLSTM::compute_beta(vector< vector<float> >& beta_matrix, 
	vector< vector<float> >& matrix)
{
	const unsigned premise_size = matrix.size();
	const unsigned hypothesis_size = matrix[0].size();
	float result=0;
	for(unsigned i=0; i<premise_size; ++i)
	{
		for(unsigned j=0; j<hypothesis_size; ++j)
		{
			result = 0;
			for(unsigned k=0; k<premise_size; ++k)
				result += exp( matrix[k][j] );
			beta_matrix[i][j] = exp( matrix[i][j] ) / result;
		}
	}
}


/**
	* \name compute_context_vector
	* \brief The context vector of the premise or the hypothesis.
	* 
	* \param cg : computation graph.
	* \param matrix : normalized attention weight matrice 
	* (alpha for the premise, else beta).
	* \param sentence_repr : vector containing the hidden 
	* representation of each premise's (or hypothesis') word.
	* \param sentence_size : number of words in the hypothesis (if the premise's context vector is initialized), 
	* else number of words in the premise 
	* \param other_sentence_size : number of words in the premise (if the premise's context vector is initialized), 
	* else number of words in the hypothesis 
	* \param context_vect : empty context vector that is initialized by the function.
*/
void BiLSTM::compute_context_vector(vector< vector<float> >& matrix, vector<Expression>& sentence_repr, 
	unsigned sentence_size, unsigned other_sentence_size, vector<Expression>& context_vect)
{
	for(unsigned j=0; j<other_sentence_size; ++j)
	{
		vector<Expression> vect;
		for(unsigned i=0; i<sentence_size; ++i)
		{
			Expression e = sentence_repr[i] * matrix[i][j];
			vect.push_back(e); 
		}
		context_vect[j] = sum(vect);
	}
}


/**
	* \name run_KIM
	* \brief Gives a vector of score for each label, using the third system.
	* 
	* \param set : the data set. 
	* \param embedding : word embeddings.
	* \param num_sentence : numero of the instance.
	* \param cg : computation graph.
	* 
	* \return A vector of score for each label for the "num_sentences"th 
	* instance. 
*/
Expression BiLSTM::run_KIM(DataSet& set, Embeddings& embedding, unsigned num_sentence, ComputationGraph& cg) 
{
	// Representation of each word (of the premise and of the hypothesis)
	// by the BiLSTM.
	vector<Expression> premise_lstm_repr;
	vector<Expression> hypothesis_lstm_repr;
	words_representation(embedding, set, true, cg, num_sentence, 
		premise_lstm_repr);
	words_representation(embedding, set, false, cg, num_sentence, 
		hypothesis_lstm_repr);

	// Creating attention matrix
	vector< vector<float> > attention_matrix(premise_lstm_repr.size(), 
		vector<float>(hypothesis_lstm_repr.size()) );
	create_attention_matrix(attention_matrix, premise_lstm_repr, 
		hypothesis_lstm_repr); 

	// Computing alpha and beta 
	vector< vector<float> > alpha_matrix(premise_lstm_repr.size(), 
		vector<float>(hypothesis_lstm_repr.size()) );
	vector< vector<float> > beta_matrix(premise_lstm_repr.size(), 
		vector<float>(hypothesis_lstm_repr.size()) );
	compute_alpha(alpha_matrix, attention_matrix);
	compute_beta(beta_matrix, attention_matrix); 

	// Computing context-vector 
	vector<Expression> premise_c_vect(premise_lstm_repr.size());
	vector<Expression> hypothesis_c_vect(hypothesis_lstm_repr.size());
	compute_context_vector(alpha_matrix, hypothesis_lstm_repr, 
		hypothesis_lstm_repr.size(), premise_lstm_repr.size(), premise_c_vect);
	compute_context_vector(beta_matrix, premise_lstm_repr, 
		premise_lstm_repr.size(), hypothesis_lstm_repr.size(), hypothesis_c_vect);

	// Mean-pooling 
	vector<Expression> pool(2);
	pool[0] = sum(premise_c_vect) / static_cast<double>(premise_lstm_repr.size());
	pool[1] = sum(hypothesis_c_vect) / static_cast<double>(hypothesis_lstm_repr.size());

	// Concat pooling 
	Expression concat = concatenate(pool);

	// Computing score 
	Expression W = parameter(cg, p_W); 
	Expression bias = parameter(cg, p_bias); 
	Expression score = affine_transform({bias, W, concat});

	return score;
}
