#include <sys/types.h>
#include <unistd.h>
#include "bilstm.hpp"

using namespace std;
using namespace dynet;

/**
 * \file rnn_fct.cpp
*/

BiLSTM::BiLSTM(unsigned nblayer, unsigned inputdim,
        unsigned hiddendim, float dropout, ParameterCollection& model) :
        nb_layers(nblayer), input_dim(inputdim), hidden_dim(hiddendim),
        dropout_rate(dropout)
{
        apply_dropout = (dropout != 0);

        forward_lstm = new VanillaLSTMBuilder(nb_layers, input_dim, hidden_dim, model); 
        backward_lstm = new VanillaLSTMBuilder(nb_layers, input_dim, hidden_dim, model);


        p_W = model.add_parameters({NB_CLASSES, 4*hidden_dim});
        p_bias = model.add_parameters({NB_CLASSES});

}

void BiLSTM::disable_dropout()
{
        apply_dropout = false;
}

void BiLSTM::enable_dropout()
{
        apply_dropout = true;
}

/* Call this fonction twice (the first time for the premise, the second time for the hypothesis) 
 * sentence_repr[i] = representation of the word nb i by the KIM method
 */
/**
        * \brief Give a representation for each word of the given sentence
        * \details Use a Bi-LSTM to run a forward and backward LSTM on the sentence. 
        * The hidden states generated by the LSTMs at each time step are concatenated.
        * 
        * \param embedding : the words embedding
        * \param set : the dataset 
        * \param sentence : The sentence you want to represent (1 if you want the premise, 2 if you want the hypothesis)
        * \param cg : the computation graph
        * \param num_sentence : the number of the sample processed
        * \param sentence_repr : matrix of size (number of words in the sentence, hidden dimension). 
        * sentence_repr[i] = representation of the i_th word
*/
void BiLSTM::words_representation(Embeddings& embedding, Data& set, unsigned sentence,
        ComputationGraph& cg, unsigned num_sentence, vector<Expression>& sentence_repr)
{
        const unsigned nb_words = set.get_nb_words(sentence, num_sentence);
        if (apply_dropout)
        { 
                forward_lstm->set_dropout(dropout_rate);
                backward_lstm->set_dropout(dropout_rate);
        }
        else 
        {
                forward_lstm->disable_dropout();
                backward_lstm->disable_dropout();
        }
        forward_lstm->new_graph(cg);  // reset LSTM builder for new graph
        forward_lstm->start_new_sequence(); //to do before add_input() and after new_graph()
        backward_lstm->new_graph(cg);  // reset LSTM builder for new graph
        backward_lstm->start_new_sequence(); //to do before add_input() and after new_graph()

        vector<Expression> tmp;
        unsigned i;
        int j;

        /* Run forward LSTM */
        for(i=0; i<nb_words; ++i)
        {
                sentence_repr.push_back(forward_lstm->add_input( embedding.get_embedding_expr(cg, set.get_word_id(sentence, num_sentence, i)) ) );
        }
        /* Run backward LSTM */
        for(j=nb_words-1; j>=0; --j)
        {
                tmp.push_back(backward_lstm->add_input( 
                        embedding.get_embedding_expr(cg, set.get_word_id(sentence, num_sentence, static_cast<unsigned>(j))) ) );
        }
        /* Concat */
        for(i=0; i<nb_words; ++i)
        {
                vector<Expression> input_expr(2);
                input_expr[0] = sentence_repr[i];
                input_expr[1] = tmp[i];
                sentence_repr[i] = concatenate(input_expr);
        }

}

void BiLSTM::compute_alpha(ComputationGraph& cg, vector< vector<float> >& alpha_matrix, vector< vector<float> >& matrix)
{
        const unsigned premise_size = matrix.size();
        const unsigned hypothesis_size = matrix[0].size();
        float result=0;
        for(unsigned i=0; i<premise_size; ++i)
        {
                for(unsigned j=0; j<hypothesis_size; ++j)
                {
                        result = 0;
                        for(unsigned k=0; k<hypothesis_size; ++k)
                                result += exp( matrix[i][k] );
                        alpha_matrix[i][j] = exp( matrix[i][j] ) / result;
                }
        }
        /*Expression e = input(cg, Dim({premise_size, hypothesis_size}), alpha_matrix);
        return e;*/
}

void BiLSTM::compute_beta(ComputationGraph& cg, vector< vector<float> >& beta_matrix, vector< vector<float> >& matrix)
{
        const unsigned premise_size = matrix.size();
        const unsigned hypothesis_size = matrix[0].size();
        float result=0;
        for(unsigned i=0; i<premise_size; ++i)
        {
                for(unsigned j=0; j<hypothesis_size; ++j)
                {
                        result = 0;
                        for(unsigned k=0; k<premise_size; ++k)
                                result += exp( matrix[k][j] );
                        beta_matrix[i][j] = exp( matrix[i][j] ) / result;

                        //cout << beta_matrix[i][j] << ' ';
                }
                //cout << endl;
        }
        /*Expression e = input(cg, {premise_size, hypothesis_size}, beta_matrix);
        return e;*/
}

void BiLSTM::create_attention_matrix(ComputationGraph& cg, vector< vector<float> >& matrix, 
        vector<Expression>& premise_lstm_repr, vector<Expression>& hypothesis_lstm_repr)
{
        const unsigned premise_size = premise_lstm_repr.size();
        const unsigned hypothesis_size = hypothesis_lstm_repr.size();
        for(unsigned i=0; i<premise_size; ++i)
        {
                for(unsigned j=0; j<hypothesis_size; ++j)
                {
                        Expression e = transpose(premise_lstm_repr[i]) * hypothesis_lstm_repr[j];
                        matrix[i][j] = as_scalar( e.value() );

                        //cout << matrix[i][j] << ' ';
                }
                //cout << endl;
        }
}

/** AVEC EXPRESSION DANS LA MATRICE D'ATTENTION */ 
/**
void BiLSTM::create_attention_matrix(ComputationGraph& cg, vector< vector<Expression> >& matrix, 
        vector<Expression>& premise_lstm_repr, vector<Expression>& hypothesis_lstm_repr)
{
        vector<Expression> tmp;
        const unsigned premise_size = premise_lstm_repr.size();
        const unsigned hypothesis_size = hypothesis_lstm_repr.size();
        for(unsigned i=0; i<premise_size; ++i)
        {
                for(unsigned j=0; j<hypothesis_size; ++j)
                {
                        Expression e = transpose(premise_lstm_repr[i]) * hypothesis_lstm_repr[j];
                        tmp.push_back( e );
                }
                matrix.push_back(tmp);
        }
}
*/
void BiLSTM::compute_a_context_vector(ComputationGraph& cg, vector< vector<float> >& alpha_matrix,
        vector<Expression>& hypothesis_lstm_repr, unsigned premise_size, vector<Expression>& a_c_vect)
{
        const unsigned hypothesis_size = hypothesis_lstm_repr.size();

        for(unsigned i=0; i<premise_size; ++i)
        {
                vector<Expression> vect;
                for(unsigned j=0; j<hypothesis_size; ++j)
                {
                        Expression e = hypothesis_lstm_repr[j] * alpha_matrix[i][j];
                        vect.push_back(e); 
                }
                a_c_vect[i] = sum(vect);
        }
}

void BiLSTM::compute_b_context_vector(ComputationGraph& cg, vector< vector<float> >& beta_matrix,
        vector<Expression>& premise_lstm_repr, unsigned hypothesis_size, vector<Expression>& b_c_vect)
{
        const unsigned premise_size = premise_lstm_repr.size();

        for(unsigned j=0; j<hypothesis_size; ++j)
        {
                vector<Expression> vect;
                for(unsigned i=0; i<premise_size; ++i)
                {
                        Expression e = premise_lstm_repr[i] * beta_matrix[i][j];
                        vect.push_back(e); 
                }
                b_c_vect[j] = sum(vect);
        }
}

Expression BiLSTM::run_KIM(Data& set, Embeddings& embedding, unsigned num_sentence, 
        ComputationGraph& cg) 
{
        /* Representation of each word (of the premise and of the hypothesis)
         * by the BiLSTM explained in the step 1 of KIM.
         */
        vector<Expression> premise_lstm_repr;
        vector<Expression> hypothesis_lstm_repr;
        words_representation(embedding, set, 1, cg, num_sentence, premise_lstm_repr);
        words_representation(embedding, set, 2, cg, num_sentence, hypothesis_lstm_repr);

        /* Creating attention matrix */
        vector< vector<float> > attention_matrix(premise_lstm_repr.size(), vector<float>(hypothesis_lstm_repr.size()) );
        create_attention_matrix(cg, attention_matrix, premise_lstm_repr, hypothesis_lstm_repr); 
        //cerr<< "Dim of attention matrix = ("<<attention_matrix.size()<<", "<< attention_matrix[0].size()<<")"<<endl;

        // Computing alpha and beta 
        vector< vector<float> > alpha_matrix(premise_lstm_repr.size(), vector<float>(hypothesis_lstm_repr.size()) );
        vector< vector<float> > beta_matrix(premise_lstm_repr.size(), vector<float>(hypothesis_lstm_repr.size()) );
        compute_alpha(cg, alpha_matrix, attention_matrix);
        compute_beta(cg, beta_matrix, attention_matrix); //softmax(e_ij);
        /*cerr<< "Dim of alpha matrix = ("<<alpha_matrix.size()<<", "<< alpha_matrix[0].size()<<")"<<endl;
        cerr<< "Dim of beta matrix = ("<<beta_matrix.size()<<", "<< beta_matrix[0].size()<<")"<<endl;*/

        // Computing context-vector 
        vector<Expression> a_c_vect(premise_lstm_repr.size());
        vector<Expression> b_c_vect(hypothesis_lstm_repr.size());
        compute_a_context_vector(cg, alpha_matrix, hypothesis_lstm_repr, premise_lstm_repr.size(), a_c_vect);
        compute_b_context_vector(cg, beta_matrix, premise_lstm_repr, hypothesis_lstm_repr.size(), b_c_vect);
        /*cerr<< "Dim of a context vector = "<<a_c_vect.size()<<endl;
        cerr<< "Dim of b context vector = "<<b_c_vect.size()<<endl;*/

        // Pooling 
        vector<Expression> pool(2);
        pool[0] = sum(a_c_vect);
        pool[1] = sum(b_c_vect);

        // Concat pooling 
        Expression concat = concatenate(pool);
        //cerr<< "Dim of the final vector = "<<concat.dim()<<endl;

        // Computing score 
        Expression W = parameter(cg, p_W); 
        Expression bias = parameter(cg, p_bias); 
        Expression score = affine_transform({bias, W, concat});

        return score;
}

/**
        * \brief Compute the negative log softmax
        * 
        * \param set : the dataset 
        * \param embedding : the words embedding
        * \param num_sentence : the number of the sample processed
        * \param cg : the computation graph
        * 
        * \return The negative log softmax as an Expression
*/
Expression BiLSTM::get_neg_log_softmax(Data& set, Embeddings& embedding, unsigned num_sentence, 
        ComputationGraph& cg)
{
        const unsigned label = set.get_label(num_sentence);
        Expression score = run_KIM(set, embedding, num_sentence, cg);
        Expression loss_expr = pickneglogsoftmax(score, label);

        return loss_expr;
}

/**
        * \brief Predict the class of 2 sentences
        * 
        * \param set : the dataset 
        * \param embedding : the words embedding
        * \param num_sentence : the number of the sample processed
        * \param cg : the computation graph
        * 
        * \return The most probable class
*/
unsigned BiLSTM::predict(Data& set, Embeddings& embedding, unsigned num_sentence, 
        ComputationGraph& cg)
{
        Expression x = run_KIM(set, embedding, num_sentence, cg);
        vector<float> probs = as_vector(cg.forward(x));
        unsigned argmax=0;

        for (unsigned k = 0; k < probs.size(); ++k) 
                if (probs[k] > probs[argmax])
                        argmax = k;

        return argmax;
}

/* Handling fonctions to training and testing steps. */

void BiLSTM::run_predict(ParameterCollection& model, Data& test_set, Embeddings& embedding, char* parameters_filename)
{
        // Output file containing the dev file's predictions ---------------------------------------------
        /*fstream predicted_file("predictions.txt", ios::in | ios::out | ios::trunc);
        if(!predicted_file)
        {
                cerr << "Problem with the prediction file \n";
                exit(EXIT_FAILURE);
        }*/

        // Load preexisting weights
        cerr << "Loading parameters ...\n";
        TextFileLoader loader(parameters_filename);
        loader.populate(model);
        cerr << "Parameters loaded !\n";

        cerr << "Testing ...\n";
        unsigned positive = 0;
        unsigned label_predicted;
        unsigned nb_of_sentences = test_set.get_nb_sentences();
        disable_dropout();
        for (unsigned i=0; i<nb_of_sentences; ++i)
        {
                ComputationGraph cg;
                label_predicted = predict(test_set, embedding, i, cg);
                if (label_predicted == test_set.get_label(i))
                        positive++;
        }

        // Print informations
        cerr << "Accuracy = " << (positive / (double) nb_of_sentences) << endl;
        //predicted_file.close();
}

void BiLSTM::run_train(ParameterCollection& model, Data& train_set, Data& dev_set, Embeddings& embedding, char* output_emb_filename, unsigned nb_epoch, unsigned batch_size)
{
        Trainer* trainer = new AdamTrainer(model);

        // Model output file
        ostringstream os;
        os << "rnn"
           << "_D-" << dropout_rate
           << "_L-" << nb_layers
           << "_I-" << input_dim
           << "_H-" << hidden_dim
           << "_pid-" << getpid() << ".params";
        string parameter_filename = os.str();
        cerr << "Parameters will be written to: " << parameter_filename << endl;

        //double best = 9e+99; //avec dev loss
        double best = 0; //avec dpos
        unsigned si;
        unsigned nb_samples;
        unsigned nb_of_sentences = train_set.get_nb_sentences();
        unsigned nb_of_sentences_dev = dev_set.get_nb_sentences();

        // Number of batches in training set
        unsigned int nb_batches = nb_of_sentences / batch_size; 
        cerr << "nb of examples = "<<nb_of_sentences<<endl;
        cerr << "batches size = "<<batch_size<<endl;
        cerr << "nb of batches = "<<nb_batches<<endl;
        cerr << "nb epochs = " << nb_epoch<<endl;

        vector<unsigned> order(nb_of_sentences);
        for (unsigned i = 0; i < nb_of_sentences; ++i) 
                order[i] = i;
        unsigned numero_sentence;
        for(unsigned completed_epoch=0; completed_epoch < nb_epoch; ++completed_epoch) 
        {
                // Reshuffle the dataset
                cerr << "\n**SHUFFLE\n";
                random_shuffle(order.begin(), order.end());
                Timer iteration("completed in");

                nb_samples=0;
                numero_sentence=0;
                enable_dropout();
                train_score(train_set, embedding, nb_batches, nb_samples, batch_size, completed_epoch, nb_of_sentences, trainer, order, numero_sentence);
                disable_dropout();
                dev_score(model, dev_set, embedding, parameter_filename, nb_of_sentences_dev, best, output_emb_filename);

        }
}

void BiLSTM::train_score(Data& train_set, Embeddings& embedding, unsigned nb_batches, 
        unsigned& nb_samples, unsigned batch_size, unsigned completed_epoch, unsigned nb_of_sentences, Trainer* trainer, vector<unsigned>& order, unsigned& numero_sentence)
{
        double loss = 0;

        for(unsigned batch = 0; batch < nb_batches; ++batch)
        {
                ComputationGraph cg; // we create a new computation graph for the epoch, not each item.
                vector<Expression> losses;

                // we will treat all those sentences as a single batch
                for (unsigned si = 0; si < batch_size; ++si) 
                {
                        Expression loss_expr = get_neg_log_softmax(train_set, embedding, order[numero_sentence], cg);
                        losses.push_back(loss_expr);
                        ++numero_sentence;
                }
                // Increment number of samples processed
                nb_samples += batch_size;

                /*  we accumulated the losses from all the batch.
                        Now we sum them, and do forward-backward as usual.
                        Things will run with efficient batch operations.  */
                Expression sum_losses = (sum(losses)) / (double)batch_size; //averaging the losses
                loss += as_scalar(cg.forward(sum_losses));
                cg.backward(sum_losses); //backpropagation gradient
                trainer->update(); //update parameters

                if((batch + 1) % (nb_batches / 160) == 0 || batch==nb_batches-1)
                {
                        trainer->status();
                        cerr << " Epoch " << completed_epoch+1 << " | E = " << (loss / static_cast<double>(nb_of_sentences))
                                 << " | samples processed = "<<nb_samples<<endl;
                }
        }
}

void BiLSTM::dev_score(ParameterCollection& model, Data& dev_set, Embeddings& embedding, string parameter_filename, unsigned nb_of_sentences_dev, double& best, char* output_emb_filename)
{        
        double dev_loss = 0;
        double positive = 0;
        unsigned label_predicted;

        for (unsigned i=0; i<nb_of_sentences_dev; ++i)
        {
                ComputationGraph cg;
                label_predicted = predict(dev_set, embedding, i, cg);
                if (label_predicted == dev_set.get_label(i))
                        positive++;
        }
        if (positive > best) //save the model if it's better than before
        {
                cerr << "it's better !\n";
                best = positive;
                TextFileSaver saver(parameter_filename);
                saver.save(model);
                if(output_emb_filename != NULL)
                        embedding.print_embedding(output_emb_filename);
        }
        cerr << "Accuracy Dev = " << (positive / (double) nb_of_sentences_dev) << endl;

}