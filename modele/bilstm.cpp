#include <sys/types.h>
#include <unistd.h>
#include "bilstm.hpp"

using namespace std;
using namespace dynet;

/**
 * \file rnn_fct.cpp
*/

BiLSTM::BiLSTM(unsigned nblayer, unsigned inputdim,
	unsigned hiddendim, float dropout, ParameterCollection& model) :
	nb_layers(nblayer), input_dim(inputdim), hidden_dim(hiddendim),
	dropout_rate(dropout)
{
	apply_dropout = (dropout != 0);
	
	forward_lstm = new VanillaLSTMBuilder(nb_layers, input_dim, hidden_dim, model); 
	backward_lstm = new VanillaLSTMBuilder(nb_layers, input_dim, hidden_dim, model);
	
	
	p_W = model.add_parameters({NB_CLASSES, (2*hidden_dim)});
	p_W_attention = model.add_parameters({(2*hidden_dim), (2*hidden_dim)});
	p_bias = model.add_parameters({NB_CLASSES});
	
}

void BiLSTM::disable_dropout()
{
	apply_dropout = false;
}

void BiLSTM::enable_dropout()
{
	apply_dropout = true;
}

/* Call this fonction twice (the first time for the premise, the second time for the hypothesis) 
 * sentence_repr[i] = representation of the word nb i by the KIM method
 */
/**
	* \brief Give a representation for each word of the given sentence
	* \details Use a Bi-LSTM to run a forward and backward LSTM on the sentence. 
	* The hidden states generated by the LSTMs at each time step are concatenated.
	* 
	* \param embedding : the words embedding
	* \param set : the dataset 
	* \param sentence : The sentence you want to represent (1 if you want the premise, 2 if you want the hypothesis)
	* \param cg : the computation graph
	* \param num_sentence : the number of the sample processed
	* \param sentence_repr : matrix of size (number of words in the sentence, hidden dimension). 
	* sentence_repr[i] = representation of the i_th word
*/
void BiLSTM::words_representation(Embeddings& embedding, Data& set, unsigned sentence,
	ComputationGraph& cg, unsigned num_sentence, vector<Expression>& sentence_repr)
{
	const unsigned nb_words = set.get_nb_words(sentence, num_sentence);
	if (apply_dropout)
	{ 
		forward_lstm->set_dropout(dropout_rate);
		backward_lstm->set_dropout(dropout_rate);
	}
	else 
	{
		forward_lstm->disable_dropout();
		backward_lstm->disable_dropout();
	}
	forward_lstm->new_graph(cg);  // reset LSTM builder for new graph
	forward_lstm->start_new_sequence(); //to do before add_input() and after new_graph()
	backward_lstm->new_graph(cg);  // reset LSTM builder for new graph
	backward_lstm->start_new_sequence(); //to do before add_input() and after new_graph()
	
	vector<Expression> tmp;
	unsigned i;
	int j;
	
	/* Run forward LSTM */
	for(i=0; i<nb_words; ++i)
	{
		sentence_repr.push_back(forward_lstm->add_input( embedding.get_embedding_expr(cg, set.get_word_id(sentence, num_sentence, i)) ) );
	}
	/* Run backward LSTM */
	for(j=nb_words-1; j>=0; --j)
	{
		tmp.push_back(backward_lstm->add_input( 
			embedding.get_embedding_expr(cg, set.get_word_id(sentence, num_sentence, static_cast<unsigned>(j))) ) );
	}
	/* Concat */
	for(i=0; i<nb_words; ++i)
		sentence_repr[i] = concatenate({sentence_repr[i],tmp[i]});
}

Expression BiLSTM::run_s4(Data& set, Embeddings& embedding, unsigned num_sentence, 
	ComputationGraph& cg) 
{
	/* Representation of each word (of the premise and of the hypothesis)
	 * by the BiLSTM explained in the step 1 of KIM.
	 */
	vector<Expression> premise_lstm_repr;
	vector<Expression> hypothesis_lstm_repr;
	words_representation(embedding, set, 1, cg, num_sentence, premise_lstm_repr);
	words_representation(embedding, set, 2, cg, num_sentence, hypothesis_lstm_repr);
	const unsigned words_premise = premise_lstm_repr.size();
	const unsigned words_hypothesis = hypothesis_lstm_repr.size();
	
	Expression x;
	Expression alpha;
	Expression concat;
	vector<Expression> scores;
	Expression W = parameter(cg, p_W); 
	Expression W_attention = parameter(cg, p_W_attention);
	Expression bias = parameter(cg, p_bias); 
	for(unsigned i=0; i<words_premise; ++i)
		for(unsigned j=0; j<words_hypothesis; ++j)
		{
			x = premise_lstm_repr[i] * W_attention;
			alpha = softmax(x * transpose(hypothesis_lstm_repr[j]));
			cerr << "Dim d'alpha = " << alpha.dim() << endl;
			concat = concatenate({premise_lstm_repr[i],hypothesis_lstm_repr[j]});
			cerr << "Dim concat mot prem + mot hyp = " << concat.dim() << endl;
			x = alpha + concat; // A REVOIR ICI
			scores.push_back(affine_transform({bias, W, x}));
		}

	return sum(scores);
}

/**
	* \brief Compute the negative log softmax
	* 
	* \param set : the dataset 
	* \param embedding : the words embedding
	* \param num_sentence : the number of the sample processed
	* \param cg : the computation graph
	* 
	* \return The negative log softmax as an Expression
*/
Expression BiLSTM::get_neg_log_softmax(Data& set, Embeddings& embedding, unsigned num_sentence, 
	ComputationGraph& cg)
{
	const unsigned label = set.get_label(num_sentence);
	Expression score = run_s4(set, embedding, num_sentence, cg);
	Expression loss_expr = pickneglogsoftmax(score, label);
	
	return loss_expr;
}

/**
	* \brief Predict the class of 2 sentences
	* 
	* \param set : the dataset 
	* \param embedding : the words embedding
	* \param num_sentence : the number of the sample processed
	* \param cg : the computation graph
	* 
	* \return The most probable class
*/
unsigned BiLSTM::predict(Data& set, Embeddings& embedding, unsigned num_sentence, 
	ComputationGraph& cg)
{
	Expression x = run_s4(set, embedding, num_sentence, cg);
	vector<float> probs = as_vector(cg.forward(x));
	unsigned argmax=0;
	
	for (unsigned k = 0; k < probs.size(); ++k) 
		if (probs[k] > probs[argmax])
			argmax = k;
	
	return argmax;
}

/* Handling fonctions to training and testing steps. */

void BiLSTM::run_predict(ParameterCollection& model, Data& test_set, Embeddings& embedding, char* parameters_filename)
{
	// Output file containing the dev file's predictions ---------------------------------------------
	/*fstream predicted_file("predictions.txt", ios::in | ios::out | ios::trunc);
	if(!predicted_file)
	{
		cerr << "Problem with the prediction file \n";
		exit(EXIT_FAILURE);
	}*/
	
	// Load preexisting weights
	cerr << "Loading parameters ...\n";
	TextFileLoader loader(parameters_filename);
	loader.populate(model);
	cerr << "Parameters loaded !\n";
	
	cerr << "Testing ...\n";
	unsigned positive = 0;
	unsigned label_predicted;
	unsigned nb_of_sentences = test_set.get_nb_sentences();
	disable_dropout();
	for (unsigned i=0; i<nb_of_sentences; ++i)
	{
		ComputationGraph cg;
		label_predicted = predict(test_set, embedding, i, cg);
		if (label_predicted == test_set.get_label(i))
			positive++;
	}
	
	// Print informations
	cerr << "Accuracy = " << (positive / (double) nb_of_sentences) << endl;
	//predicted_file.close();
}

void BiLSTM::run_train(ParameterCollection& model, Data& train_set, Data& dev_set, Embeddings& embedding, char* output_emb_filename, unsigned nb_epoch, unsigned batch_size)
{
	Trainer* trainer = new AdamTrainer(model);
	
	// Model output file
	ostringstream os;
	os << "rnn"
	   << "_D-" << dropout_rate
	   << "_L-" << nb_layers
	   << "_I-" << input_dim
	   << "_H-" << hidden_dim
	   << "_pid-" << getpid() << ".params";
	string parameter_filename = os.str();
	cerr << "Parameters will be written to: " << parameter_filename << endl;
	
	//double best = 9e+99; //avec dev loss
	double best = 0; //avec dpos
	unsigned si;
	unsigned nb_samples;
	unsigned nb_of_sentences = train_set.get_nb_sentences();
	unsigned nb_of_sentences_dev = dev_set.get_nb_sentences();
	
	// Number of batches in training set
	unsigned int nb_batches = nb_of_sentences / batch_size; 
	cerr << "nb of examples = "<<nb_of_sentences<<endl;
	cerr << "batches size = "<<batch_size<<endl;
	cerr << "nb of batches = "<<nb_batches<<endl;
	cerr << "nb epochs = " << nb_epoch<<endl;
	
	vector<unsigned> order(nb_of_sentences);
	for (unsigned i = 0; i < nb_of_sentences; ++i) 
		order[i] = i;
	unsigned numero_sentence;
	for(unsigned completed_epoch=0; completed_epoch < nb_epoch; ++completed_epoch) 
	{
		// Reshuffle the dataset
		cerr << "\n**SHUFFLE\n";
		random_shuffle(order.begin(), order.end());
		Timer iteration("completed in");

		nb_samples=0;
		numero_sentence=0;
		enable_dropout();
		train_score(train_set, embedding, nb_batches, nb_samples, batch_size, completed_epoch, nb_of_sentences, trainer, order, numero_sentence);
		disable_dropout();
		dev_score(model, dev_set, embedding, parameter_filename, nb_of_sentences_dev, best, output_emb_filename);
		
	}
}

void BiLSTM::train_score(Data& train_set, Embeddings& embedding, unsigned nb_batches, 
	unsigned& nb_samples, unsigned batch_size, unsigned completed_epoch, unsigned nb_of_sentences, Trainer* trainer, vector<unsigned>& order, unsigned& numero_sentence)
{
	double loss = 0;
	
	for(unsigned batch = 0; batch < nb_batches; ++batch)
	{
		ComputationGraph cg; // we create a new computation graph for the epoch, not each item.
		vector<Expression> losses;
		
		// we will treat all those sentences as a single batch
		for (unsigned si = 0; si < batch_size; ++si) 
		{				
			Expression loss_expr = get_neg_log_softmax(train_set, embedding, order[numero_sentence], cg);
			losses.push_back(loss_expr);
			++numero_sentence;
		}
		// Increment number of samples processed
		nb_samples += batch_size;
		
		/*  we accumulated the losses from all the batch.
			Now we sum them, and do forward-backward as usual.
			Things will run with efficient batch operations.  */
		Expression sum_losses = (sum(losses)) / (double)batch_size; //averaging the losses
		loss += as_scalar(cg.forward(sum_losses));
		cg.backward(sum_losses); //backpropagation gradient
		trainer->update(); //update parameters
		
		if(batch==nb_batches-1)
		{
			trainer->status();
			cerr << " Epoch " << completed_epoch+1 << " | E = " << (loss / static_cast<double>(nb_of_sentences))
				 << " | samples processed = "<<nb_samples<<endl;
		}
	}	
}

void BiLSTM::dev_score(ParameterCollection& model, Data& dev_set, Embeddings& embedding, string parameter_filename, unsigned nb_of_sentences_dev, double& best, char* output_emb_filename)
{	 
	double dev_loss = 0;
	double positive = 0;
	unsigned label_predicted;
	
	for (unsigned i=0; i<nb_of_sentences_dev; ++i)
	{
		ComputationGraph cg;
		label_predicted = predict(dev_set, embedding, i, cg);
		if (label_predicted == dev_set.get_label(i))
			positive++;
	}
	if (positive > best) //save the model if it's better than before
	{
		cerr << "it's better !\n";
		best = positive;
		TextFileSaver saver(parameter_filename);
		saver.save(model);
		if(output_emb_filename != NULL)
			embedding.print_embedding(output_emb_filename);
	}
	cerr << "Accuracy Dev = " << (positive / (double) nb_of_sentences_dev) << endl;
	
}
