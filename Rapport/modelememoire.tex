

% Les packages utilise's ci-dessous le sont a` titre indicatif ;
% vous pouvez les changer a` votre convenance.

% Le type de document: article, rapport...
\documentclass[a4paper]{article}

% Mettre les diffe'rents packages et fonctions que l'on utilise
%\usepackage[english]{babel}
\usepackage[french]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage[nottoc, notlof, notlot]{tocbibind}
\usepackage{appendix}
\usepackage{comment}
\usepackage{multirow}


% Commenter l'une de ces deux lignes
%\RequirePackage[applemac]{inputenc}
\RequirePackage[latin1]{inputenc}
\usepackage[T1]{fontenc}

\begin{document}


%----------- A   C O M P L E T E R   P A R   L E S   A U T E U R S ------------


% Titre du rapport
\def\TitreRapport{
    Interprétation de prédictions de modèles pour l'inférence textuelle 
    en perturbant significativement les entrées
}

% Pre'nom et nom de l'autrice
\def\NomsAuteurs{
    Marjorie Armando
}

% Date du rapport (dans la me^me langue que le titre)
\def\DateRapport{
    1 juin 2018
}

% Nom des encadrants
\def\Encadrants{
    \textbf{Encadrant} \\
    Benoit Favre
}
% Nom du laboratoire
\def\Labo{
    Laboratoire d'Informatique et des Systèmes - LIS
}



% Re'sume' en franc,ais avec mots-cle's
\def\ResumeFrancais{
    La reconnaissance de l'inférence textuelle (Recognizing Textual Entailment : RTE) est au coeur de tous les aspects de 
    la compréhension de texte en traitement automatique du langage (TAL). 
    Le but de la RTE est de savoir automatiquement si une phrase, appelée l'hypothèse, est 
    déduite d'une autre phrase, appelée la prémisse.
    En utilisant des réseaux de neurones complexes, nous pouvons obtenir de bons taux de réussite pour la RTE. Cependant, 
    ces réseaux ne sont pas interprétables, ainsi nous n'avons pas la possibilité de savoir si le modèle s'est basé sur 
    de bonnes informations pour sa décision. 
    Dans ce travail, nous proposons la méthode Best Adversarial eXample for Interpretability (BAXI), qui respecte les règles d'une "bonne" 
    explication pour rendre un modèle interprétable dans le cadre de la RTE, 
    avec l'utilisation du corpus SNLI. Nous allons la comparer 
    à la méthode Local Interpretable Model-agnostic Explanations (LIME) qui permet 
    d'expliquer les prédictions de n'importe quel classifieur en apprenant localement un modèle interprétable dans le voisinage
    de l'entrée. Pour cela, nous avons extrait un échantillon du corpus de test de 
    SNLI, et demandé à six annotateurs de donner les mots expliquant l'étiquette associée à la paire prémisse/hypothèse. 
    Grâce à ce nouveau corpus, nous mesurons le taux de la qualité de l'explication fournit 
    par la méthode BAXI et la méthode LIME, sur plusieurs systèmes 
    que nous avons implémenté.
    \\[2mm]
    {\bf Mots-clés : } inférence textuelle, apprentissage automatique,\\traitement automatique des langues naturelles, 
    interprétabilité, LSTM
}


\thispagestyle{empty}
\begin{center}
\baselineskip=1.3\normalbaselineskip
{\bf\Large \TitreRapport}\\[8mm]
{\bf\large \NomsAuteurs}\\[1mm]
{\Labo}\\[4mm]
\Encadrants\\[10mm]

{\bf Résumé}
\end{center}

\ResumeFrancais\\[4mm]

\newpage
%\tableofcontents
%\newpage
%\listoftables
%\listoffigures
%\newpage

%-------------------- T E X T E   D U   R A P P O R T -------------------------

\section{Introduction} % SECTION 1 FINI
	\subsection{Contexte et motivations}
		Dans la pratique, il y a souvent un compromis entre le taux de réussite et l'interprétabilité du modèle. 
		Certains modèles tels que les arbres de décisions ou encore les modèle linéaires sont facilement interprétables, 
		et sont donc parfois utilisés à la place de modèles complexes tels que les réseaux de neurones profonds, 
		même si ceux-ci peuvent donner de meilleurs résultats.\\
		Pouvoir interpréter un modèle permettrait de le rendre utilisable dans des domaines où les décisions doivent être 
		mûrement réfléchies, telle que la médecine. Si un modèle propose de donner un certain traitement à un patient, 
		il faut que le modèle puisse donner de bonnes explications pour que le docteur l'approuve, car les conséquences pourraient 
		être catastrophique.\\
		\\
		Dans le cadre de la reconnaissance de l'inférence textuelle (Recognizing Textual Entailment : RTE), les modèles obtenant les meilleurs taux de réussite, décrits dans la section 3.1, sont non interprétables. 
		Le problème qui se pose avec l'utilisation de modèles non interprétables est qu'ils fournissent 
		uniquement les probabilités de chaque étiquette. Comment savoir, de manière humainement compréhensible, pourquoi le modèle a 
		associé une étiquette particulière à une certaine entrée ?\\
		\\
		Pourtant, avoir un modèle digne de confiance permettrait au modèle d'être d'avantage utilisé : en effet, il a été observé, par exemple, que le fait de fournir des 
		explications augmente l'acceptation des recommandations de films \cite{ref2}. De plus, cela permettrait également au développeur de choisir 
		un modèle parmi ceux qu'il a implémenté, 
		car le taux de réussite n'est pas le seul critère à prendre en compte : un modèle peut fournir l'étiquette attendue 
		en se basant sur de mauvaises informations.\\
		Pour qu'un modèle soit digne de confiance, il faut qu'un grand nombre d'explications fournies soit jugé correcte par les 
		utilisateurs.

	\subsection{Tâches}
		Les concepts sémantiques d'inférence et de contradiction sont au coeur de tous les aspects de la compréhension de texte en TAL. 
		Ainsi, la RTE est essentielle dans des tâches telles 
		que la recherche d'information, le raisonnement de bon sens, ou encore le système 
		de question/réponse.\\
		\\
		Plus spécifiquement, soit une paire de phrase prémisse/hypothèse, la RTE se voit comme 
		objectif de détecter si la seconde phrase est en contradiction, se déduit ou bien est neutre par rapport à la 
		première phrase. Il y a donc trois étiquettes permettant d'illustrer la 
		relation entre la prémisse et l'hypothèse : \\
		\begin{itemize}\renewcommand{\labelitemi}{$\bullet$}
			\item Contradiction : l'hypothèse contredit la prémisse. Par exemple :\\
				\textit{
				Prémisse : "Le chat est entièrement blanc."\\ 
				Hypothèse : "Le chat est entièrement noir."}\\
			\newpage
			\item Neutre : l'hypothèse est possible dans le contexte de la prémisse. \\Par exemple :\\
				\textit{
				Prémisse : "Le chat dort sur la banquette."\\
				Hypothèse : "Le chat aime le chocolat."}\\

			\item Inférence : l'hypothèse est déduite de la prémisse. Par exemple :\\
				\textit{
				Prémisse : "Le chat aimerait manger la souris."\\
				Hypothèse : "Le chat a faim."}\\
		\end{itemize} \\
		\\
		\\
		Dans ce travail, nous proposons d'interpréter les prédictions des modèles axés sur la RTE avec la méthode Best Adversarial eXample for Interpretability (BAXI). 
		Nous allons comparer ses performances avec 
		la méthode Local Interpretable Model-agnostic Explanations (LIME) \cite{ref2}, grâce à un corpus explicatif que nous avons créé à l'aide de six annotateurs.\\
		\\	
		Nous allons tout d'abord lister nos hypothèses scientifiques en section 2, puis énoncer l'état de l'art en section 3 pour la RTE ainsi que pour l'interprétabilité de prédictions de modèle en général. Nous verrons l'approche de LIME en section 3.3. 
		Nous allons ensuite décrire ce qu'est une "bonne" explication pour savoir sur quoi faut-il se baser pour implémenter une méthode d'interprétabilité en section 4. Puis, nous décrirons les modèles que nous avons implémentés pour la RTE en section 5. 
		Ces modèles seront les boîtes noires que nous tenterons d'interpréter. Enfin, nous allons décrire la méthode BAXI en section 6 puis le cadre expérimental en section 7 pour donner les différents résultats obtenus en section 8. 
		Nous conclurons sur une discussion et sur les différentes perspectives en section 9 et 10.


		\begin{comment}
		Par ailleurs, on peut interpréter de manière globale en expliquant le comportement général du modèle, ou de manière locale en expliquant 
		pourquoi le modèle a choisi tel label. Notre objectif se place dans le cas de l'interprétation locale : le but est d'expliquer 
		pourquoi le modèle a choisi un label $y$ pour une entrée $x$.\\
		\\
		Dans ce travail, nous allons tout d'abord énumérer quelques méthodes existantes pour la RTE et pour l'interprétation de prédictions d'un modèle.
		Nous allons ensuite définir ce que nous devons attendre d'une "bonne" explication, puis spécifier les modèles que nous avons implémenté pour la RTE ainsi que les corpus utilisés. 
		Puis, nous allons expliciter notre technique pour l'interprétation d'une prédiction d'un modèle, et nous allons la comparer avec LIME grâce à une nouvelle méthode 
		mesurant le taux d'explications correctes. Enfin, nous allons conclure avec les différentes perspectives. 
		\end{comment}
		%\bigskip
		
\section{Hypothèse scientifique} % SECTION 2 PRESQUE FINI
	Notre principale hypothèse étudiée lors de ce travail est que BAXI repère de meilleurs mots explicatifs que LIME 
	pour la tâche de la RTE : 
	BAXI $\underset{RTE}{>}$ LIME.\\
	Un mot explicatif est un mot ayant une grande importance pour l'étiquette considérée. 
	La comparaison est effectuée à l'aide d'un corpus explicatif contenant 
	les mots explicatifs dans la prémisse et dans l'hypothèse (voir section 7.3). 

\section{Etat de l'art} % SECTION 3
	\subsection{Dans la RTE}
		Les premiers travaux sur la RTE ont été 
		formé sur de très petits ensembles de données avec des méthodes conventionnelles, 
		telles que les méthodes peu profondes \cite{ref9} ou encore
		les méthodes de logique naturelle \cite{ref10}.
		\\
		Ces dernières années, la tâche de la RTE a connu une nette amélioration, en particulier 
		grâce à la publication du corpus SNLI (the Stanford Natural Language Inference) qui contient 
		570K paires de phrases annotées \cite{ref}. Cela a permit d'entrainer des réseaux de neurones 
		complexes qui nécessitent une quantité relativement importante de données. En 
		effet, la recherche sur l'apprentissage automatique dans ce domaine été limitée 
		par le manque de corpus assez grand.\\
		\\
		
		/*DECRIRE UNE APPROCHE*/
	\subsection{Dans l'interprétabilité de prédictions de modèles}
		Plusieurs méthodes existent pour l'interprétation de prédictions de modèles, mais toutes ne s'appliquent pas au texte. La méthode 
		de Fong et Vedaldi \cite{ref25} en est un exemple : les auteurs perturbent une image en appliquant un masque par dessus pour 
		flouter quelques parties, puis voient si la probabilité de l'étiquette initiale a baissé. Si c'est le cas, alors l'objet flouté 
		dans l'image est un facteur explicatif à la décision du modèle.\\
		\\
		D'autres méthodes d'interprétation existent, telle que la méthode Deep Learning Important FeaTures (DeepLIFT) pour l'apprentissage profond, 
		qui décompose la prédiction d'un 
		réseau de neurones sur une entrée spécifique en rétropropageant les contributions de tous les neurones du réseau à 
		chaque \textit{feature} de l'entrée. DeepLIFT compare l'activation de chaque neurone à son "activation de référence" et 
		attribue des scores de contribution en fonction de la différence. L'activation de référence est choisie par 
		l'utilisateur \cite{deeplift}.\\
		On peut également citer la méthode de propagation de pertinence par couche pour l'interprétation des 
		réseaux de neurones profonds, une méthode équivalente à DeepLIFT avec l'activation de référence de tous les neurones 
		fixée à 0 \cite{ref26}.\\
		\\
		Une autre approche est la méthode SHapley Additive exPlanations (SHAP) \cite{shap} : 
		elle explique la prédiction de n'importe quel modèle en utilisant les valeurs de Shapley, introduites dans la théorie des jeux coopératives en 1953 \cite{shapley}.\\
		SHAP attribue des paiements aux joueurs en fonction de leur contribution au paiement total. Les joueurs coopèrent dans une coalition et obtiennent un certain gain de cette coopération.\\
		Ici, le "jeu" est la tâche de prédiction pour une seule instance. Le "gain" est la prédiction réelle pour cette instance moins la prédiction moyenne de toutes les instances. 
		Les "joueurs" sont les valeurs des \textit{features} présentes dans l'instance (formant une coopération), qui collaborent pour recevoir le gain.
		La valeur de Shapley d'une valeur pour une \textit{feature} est la contribution marginale moyenne de cette valeur de \textit{feature} sur toutes les coopérations de \textit{features} possibles.\\
		\\
		Ces valeurs ont récemment été utilisées pour attribuer une mesure d'importance aux \textit{features} \cite{ref5}. SHAP appartient à la classe des "méthodes d'attribution de \textit{features} additives" : 
		elle attribue une valeur à chaque valeur de \textit{features} pour chaque prédiction (c'est-à-dire une attribution de \textit{feature}). 
		Plus la valeur est haute, plus l'attribution de la \textit{feature} à la prédiction spécifique est grande. Un bon exemple est décrit dans le livre \textit{"Interpretable Machine Learning"} 
		de Molnar en section 5.7 \cite{ref5}.
		\\
		SHAP est la seule méthode qui respecte les trois propriétés suivantes, ce qui en fait la seule méthode basée sur une théorie solide :\\
		\begin{itemize}\renewcommand{\labelitemi}{$\bullet$}
			\item Précision locale : les explications sont fidèles et véridiques au modèle.\\
			\item \textit{Feature} manquante : les \textit{features} retirées n'ont aucun impact attribué aux prédictions du modèle.\\
			\item Cohérence : les explications sont cohérentes avec l'intuition humaine. Techniquement, la cohérence indique que si un modèle change de sorte que la 
			contribution d'une entrée augmente ou reste la même indépendamment des autres entrées, l'attribution de cette entrée ne devrait pas diminuer.\\
		\end{itemize}\\
		\\
		Le calcul de ces valeurs est cependant très coûteux : il faut générer toutes les coopérations possibles pour une instance, ce qui en fait une complexité 
		exponentielle et n'est donc pas utilisable dans la pratique. 
		Un algorithme efficace a été mis au point par les auteurs de SHAP, cependant il fonctionne uniquement sur les modèles basés sur les arbres.
		\\
		\\
		Concernant l'interprétabilité dans la tâche de la RTE, ce n'est que très récemment qu'une méthode a été proposée, mais uniquement 
		pour expliquer l'inférence \cite{ref21}. 
		Cette méthode entraîne un classifieur pour qu'il puisse apprendre automatiquement la relation sémantique entre   
		les mots de la prémisse et l'hypothèse grâce à WordNet. Pour interpréter la décision de l'inférence, 
		il faut naviguer dans le graphe WordNetGraph, créé par les auteurs à partir des définitions de WordNet. Voici un exemple :
		\begin{center}
		\textit{
		Prémisse : "Many cellphones have built-in digital cameras."\\
		Hypothèse : "Many cellphones can take pictures."}\end{center}\\
		Premièrement, la méthode recherche des paires de mots qui ont une forte relation sémantique et qui peuvent 
		prouver que cette inférence est vraie, puis ces paires sont envoyées en entrée à l'algorithme de navigation 
		graphique. Dans cet exemple, la meilleure paire est "\textit{digital camera}" (la source) et "\textit{pictures}" (la cible). 
		En partant de la source, la méthode récupère tous les noeuds de 
		WordNetGraph, et calcule la similarité sémantique entre chaque noeud et la cible et choisit le noeud qui a la 
		valeur la plus élevée comme prochain noeud à visiter. Cela est fait récursivement jusqu'à atteindre la cible. 
		Les segments suivants sont trouvés par l'algorithme de navigation :
		\begin{center}
		\textit{
		<digital camera has\_supertype camera>\\
		<camera has\_supertype equipment>\\
		<equipment has\_diffqual for taking photographs>}\\
		\end{center}
		où le premier segment signifie que "\textit{digital camera}" appartient à la classe "\textit{camera}", le deuxième segment 
		signifie que "\textit{camera}" appartient à la classe "\textit{equipment}", et le dernier segment signifie que "\textit{equipment}" a la 
		qualité de pouvoir prendre des photographies ("\textit{for taking photographs}").\\
		"\textit{photographs}" et "\textit{pictures}" sont synonymes, donc la recherche dans le graphe s'arrête ici, 
		et confirme bien l'inférence. Les explications suivantes, construites à partir des segments, sont alors données : 
		\begin{center}
		\textit{
		A digital camera is a kind of camera\\
		A camera is an equipment for taking photographs\\
		Photograph is synonym of picture}
		\end{center}
	\subsection{Description de LIME}
		Nous étudions dans ce travail la méthode LIME qui permet d'expliquer les prédictions de n'importe quel classifieur 
		ou regresseur. L'objectif global de LIME est d'identifier un modèle interprétable parmi le voisinage de l'entrée $x$.\\
		\\ 
		Tout d'abord, les features utilisées et les représentations interprétables des features sont à distinguer. Par exemple, 
		les features sont les embeddings des mots et la représentation interprétable de ces features est un vecteur binaire 
		qui indique la présence ou l'absence des mots.\\
		\\
		LIME définit une explication par un modèle $g \in G$, où $G$ est la classe des modèles interprétables tels que 
		les modèles linéaires ou les arbres de décisions. Vu que les modèles interprétables n'ont pas tous la même difficulté 
		à être interprété, LIME définit $\Omega(g)$ qui est une mesure de la complexité d'interpréter $g$. En 
		prenant l'example des arbres de décisions, $\Omega(g)$ est la profondeur.\\
		On dénote par $f : \mathbb{R}^{d} \rightarrow \mathbb{R}$ le modèle utilisé comme une boîte noire. $f(x)$ est la 
		probabilité que l'entrée $x$ appartienne à une certaine étiquette.\\
		LIME va alors se baser sur la représentation interprétable des données en retirant un ou plusieurs mots au hasard. Cette 
		nouvelle entrée est notée $z$. LIME définit la localité de $x$ avec $\pi_x(z)$ qui est une mesure de proximité entre $z$ et 
		$x$. C'est un noyau se basant sur la similarité cosinus.\\
		Enfin, LIME définit $\mathcal{L}(f, g, \pi_x)$ qui est une mesure pour savoir à combien $g$ est infidèle à $f$ dans la localité 
		défini par $\pi_x$. Pour préserver à la fois l'interprétabilité et la fidèlité locale, LIME minimise $\mathcal{L}(f, g, \pi_x)$ 
		avec $\Omega(g)$ assez petit pour être interprétable par les humains. L'explication de LIME est donc la suivante :
		\begin{equation}
		\mathcal{E}(x) = \quad \underset{g \in G}{argmin}\quad \mathcal{L}(f, g, \pi_x) + \Omega(g)
		\end{equation}		
		Cette formule peut être utilisée par différents modèles $g \in G$, fonctions de 
		fidèlité $\mathcal{L}(f, g, \pi_x)$, et mesure de complexité $\Omega(g)$.\\
		\\
		LIME peut alors donner les $K$ mots les plus importants de l'entrée $x$ pour toutes étiquettes.\\
		La figure ci-dessous est un exemple illustant le principe de LIME :
		
		\begin{figure}[h]
			\center
			\includegraphics[width=8cm, height=6cm, keepaspectratio]{lime_exemple.png}
			\caption[Exemple présentant l'intuition de LIME.]{\bf{Exemple présentant l'intuition de LIME. La décision de la boîte 
			noire $f$, inconnu par LIME, est représentée par le fond bleu et rose. 
			La croix rouge en gras est l'entrée $x$ que l'on veut expliquer. LIME crée des entrées modifiées, 
			utilise $f$ pour avoir la probabilité de ces entrées pour le label $y$, et les pondère par leur proximité 
			par rapport à $x$ (les poids sont représentés par la taille). La droite pointillée est l'explication apprise qui est 
			localement fidèle.}}
		\end{figure}
		\newpage
		\\
		Concernant l'évaluation des explications de LIME pour savoir si la méthode fournit des explications correctes, les auteurs 
		ont utiliser LIME /*A FINIR */.
		
	%\subsection{Dans l'interprétabilité de la RTE}
	%	Ce n'est que très recemment que Silva et al. \cite{ref21} 
		
	%	/*PARLER ARTICLE LREC*/
		
	

\section{Définition d'interprétabilité} % SECTION 4 FINI
	Il n'y a malheureusement pas de consensus concernant la définition d' "interprétabilité". 
	Miller définit cela comme étant le degré auquel un humain peut comprendre la cause d'une décision \cite{ref4}. 
	Un système a donc une meilleure interprétabilité qu'un autre si ses explications sont plus faciles à comprendre par un humain.
	\begin{comment}
	\begin{figure}[h]
		\center
		\includegraphics[width=10cm, height=6cm, keepaspectratio]{xai-figure.png}
		\caption[Concept des intelligences artificielles explicables]{Concept des intelligences artificielles explicables (eXplainable Artificial Intelligences : XAI) \cite{ref22}.}
	\end{figure}	
	\end{comment}
	\subsection{Qu'est-ce-qu'une explication ?}
		La définition donnée par Miller est assez simple : une explication est une réponse à une question commençant par "pourquoi". 
		Une question commençant par "comment" peut être retournée en une question commençant par "pourquoi".
		Le terme "explication" désigne le processus social et cognitif d'expliquer, mais c'est également le produit de ces processus.
	\subsection{Qu'est-ce-qu'une "bonne" explication ?}
		La définition d'une bonne explication ne doit pas se baser sur l'intuition de l'auteur, mais plutôt sur des faits. 
		Miller résume ce qu'est une bonne explication, c'est-à-dire ce que les humains attendent d'une explication \cite{ref5}. 
		BAXI doit suivre les trois règles suivantes, caractérisant une bonne explication :
		\paragraph*{Explication contrastée}
			C'est une explication qui doit être comparée. Les utilisateurs se demandent généralement pourquoi cette prédiction a 
			été faite et pas une autre, via la question "quelle aurait été la prédiction si cette entrée avait été changé 
			par une autre ?".\\
			Un docteur se demandant "pourquoi ce traitement ne marche pas sur ce patient ?" voudrait comparer les données de ce patient 
			à un autre patient ayant des caractéristiques similaires mais pour qui le traitement marche.\\
			La meilleure explication pour ce type d'explication est celle qui met en évidence les différences entre l'entrée traitée et 
			l'entrée de comparaison.\\
			L'entrée de comparaison peut être artificielle.
		\paragraph*{Explication sélective}
			C'est une explication qui doit être courte. Généralement, un phénomène s'explique par plusieurs facteurs. 
			Il faut en donner peu, à savoir deux ou trois raisons, même si les explications 
			peuvent être plus complexes que cela.
		\paragraph*{Explication sociale}
			Comme nous l'avons expliqué ci-dessus, une explication est un processus social, 
			c'est-à-dire qu'il faut prendre en compte les connaissances de la personne à qui l'on veut donner une explication. 
			Dans notre travail, nous partons du principe qu'une explication doit être comprise par tout le monde, 
			que ce soit par un expert du domaine de l'apprentissage automatique ou bien par quelqu'un qui n'en a jamais entendu parler.
	\begin{comment}
	\subsubsection{Explication anormale}
		Les explications "anormales" sont beaucoup appréciées, c'est-à-dire que si une cause rare a influencé la prédiction, 
		il faut la spécifier. Dans notre projet, une cause rare peut être un mot -ou un groupe de mot- de l'hypothèse qui ne peut pas 
		être mis en relation avec un mot -ou un groupe de mot- de la prémisse, mais qui a influencé la prédiction.
	\end{comment}
	

\section{Approches pour la RTE} % SECTION 5 FINI
	Nous avons implémenté trois systèmes différents avec la librairie DyNet \cite{ref6}. 
	Nous utilisons les LSTMs et les BILSTMs.\\
	
	\paragraph*{Premier système}
		Le premier système passe la prémisse et l'hypothèse au LSTM pour avoir une représentation pour chacune de ces deux phrases.  
		On les concatène pour les envoyer ensuite à une couche de décision : 
		\begin{equation}
		y = softmax( W \times [ LSTM(\textit{prémisse}) ; LSTM(\textit{hypothèse}) ] + b )
		\end{equation}
		où $y$ est un vecteur contenant les probabilités de chaque étiquette, $W$ est la matrice de poids, $LSTM(\textit{prémisse})$ et $LSTM(\textit{hypothèse})$ sont respectivement la représentation de 
		la prémisse et de l'hypothèse, $b$ est le biais, et [;] dénote la concaténation.
	\paragraph*{Deuxième système}
		Le deuxième système effectue le même mécanisme que le premier système pour avoir une représentation de la prémisse et de 
		l'hypothèse. On compare les deux représentations pour envoyer la comparaison à une couche de décision :
		\begin{equation}
		y = softmax( W \times ( LSTM(\textit{prémisse}) \times LSTM(\textit{hypothèse})^{T} ) + b )
		\end{equation}
		où $T$ dénote la transposée.
	
	\paragraph*{Troisième système}
		Le troisième système est inspiré de la méthode KIM \cite{ref3}.\\ 
		On représente les mots de la prémisse et de l'hypothèse en les passant dans un BILSTM qui utilise un LSTM \textit{forward} pour 
		lire la phrase de gauche à droite, puis un LSTM \textit{backward} pour lire la phrase dans l'autre sens. A chaque mot lu, un 
		état caché est généré par les deux LSTMs. Ces deux états cachés sont alors concaténés pour obtenir une 
		représentation du mot :\\
		$h_t = [h_t^{\rightarrow} ; h_t^{\leftarrow}]$, 
		où $h_t^{\rightarrow}$ est l'état caché généré par le LSTM \textit{forward} à 
		l'instant $t$, $h_t^{\leftarrow}$ est celui généré par le LSTM \textit{backward} à l'instant $t$, et $h_t$ est la 
		représentation du mot $t$.\\
		On dénote par $p^s$ (respectivement $h^s$) le vecteur de représentation des mots de la prémisse 
		(respectivement de l'hypothèse).\\
		Nous construisons ensuite une matrice d'alignement comme suit :
		\begin{equation}
		e_{ij} = (p_i^s)^T h_j^s
		\end{equation}
		où $p_i^s$ est la représentation du i\up{ème} mot de la prémisse, $h_j^s$ est celle du j\up{ième} mot de l'hypothèse.\\
		Avec cette matrice, nous pouvons alors construire les vecteurs de contexte $p^c$ et $h^c$ suivants pour la prémisse et l'hypothèse :
		\begin{equation}
		\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{N} exp(e_{ik})} , p_i^c = \sum_{j=1}^{N} \alpha_{ij} h_j^s   
		\end{equation}
		\begin{equation}
		\beta_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{M} exp(e_{kj})} , h_j^c = \sum_{i=1}^{M} \beta_{ij} p_i^s   
		\end{equation}
		où $M$ est la longueur de la prémisse, $N$ est la longueur de l'hypothèse, $\alpha \in \mathbb{R}^{M \times N}$ est un $softmax(e)$ 
		sur la prémisse, et $\beta \in \mathbb{R}^{M \times N}$ est un $softmax(e)$ sur l'hypothèse. Ceci permet à la prémisse de voir le contexte de l'hypothèse 
		et vice-versa.\\
		Avec ces nouvelles représentations pour les mots, on effectue du \textit{mean-pooling} :
		\begin{equation}
		pool_p = \frac{\sum_{i=1}^{N} p_i^c}{N} , pool_h = \frac{\sum_{i=1}^{M} h_i^c}{M}
		\end{equation}
		puis on effectue une concaténation du \textit{mean-pooling} de la prémisse et de l'hypothèse pour l'envoyer à une couche de décision :
		\begin{equation}
		y = softmax( W \times [ pool_p ; pool_h ] + b )
		\end{equation}	
	\subsection{Résultats des systèmes}	
		Les tables ci-dessous montrent les différents résultats des taux de réussite des étiquettes prédites avec ces 
		systèmes : \newpage
		\begin{table}[h]
			\begin{center}
			\begin{tabular}{|c|c|c|c|c|}
			\hline
			\small{Système} & \small{\% Inférence} & \small{\% Neutre} & \small{\% Contradiction} & \small{\% Dev} \\
			\hline
			1 & 73,90\% & 68,38\% & 67,33\% & 69,89\% \\
			\hline
			2 & \bf{87,98\%} & \bf{74,99\%} & \bf{78,55\%} & \bf{80,57\%} \\
			\hline
			3 & 74,92\% & 69,55\% & 68,73\% & 71,09\% \\
			\hline
			\end{tabular}
			\caption{\bf{Résultats des taux de réussite des étiquettes prédites pour le fichier de validation.}}
			\end{center}
		\end{table}

		\begin{table}[h]
			\begin{center}
			\begin{tabular}{|c|c|c|c|c|}
			\hline
			\small{Système} & \small{\% Inférence} & \small{\% Neutre} & \small{\% Contradiction} & \small{\% Test} \\
			\hline
			1 & 74,35\% & 68,13\% & 65,13\% & 69,27\% \\
			\hline
			2 & \bf{86,10\%} & \bf{76,14\%} & \bf{77,70\%} & \bf{80,07\%} \\
			\hline
			3 & 74,26\% & 69,18\% & 67,57\% & 70,39\% \\
			\hline
			\end{tabular}
			\caption{\bf{Résultats des taux de réussite des étiquettes prédites pour le fichier de test.}}
			\end{center}
		\end{table}

\section{Description de l'approche BAXI} % SECTION 6 FINI
	Cette section décrit notre technique pour interpreter une prédiction d'une entrée. L'objectif est de donner des explications pour chaque étiquette 
	en donnant les $\alpha$ mots les plus importants dans la prémisse et les $\beta$ mots les plus importants dans l'hypothèse. On parle alors "d'expliquer une étiquette". 
	On veut donc calculer l'importance de chaque mot. 
	L'importance d'un mot correspond à l'importance de sa contribution pour l'étiquette $y$.\\
	\\
	Notre intuition est la suivante : 
	On veut donner une explication pour l'étiquette $y$ et l'entrée $x$ composée de la prémisse et de l'hypothèse. 
	On remplace alors un mot et on demande à notre modèle de nous donner les probabilités de chaque étiquette avec cette nouvelle entrée. Si la probabilité 
	de l'étiquette $y$ a baissé, alors le mot était important : cela veut dire que le mot avait contribué à l'étiquette $y$. A l'inverse, si elle a augmenté, le mot n'avait donc pas contribué à $y$. 
	De plus, si la probabilité des autres étiquettes a augmenté, alors le mot a d'autant plus d'importance : en le remplaçant, l'entrée $x$ a basculé vers une autre étiquette.\\
	\\
	Pour résumé, lorsque l'on remplace $m_i$, nous pénalisons l'augmentation de la probabilité de l'étiquette que l'on veut 
	expliquer, et nous encourageons l'augmentation des probabilités des autres étiquettes.
	\\
	L'importance d'un mot est une fonction $IMP : \mathbb{R}^{d_e} \rightarrow \mathbb{R}$ comme suit : \\
	
	\begin{equation}
	impact^{y_{ref}}(m_i) = -p(y_{ref} \: | \: x \leftarrow m_i = m_a) \: + \: p(y_{ref} \: | \: x)
	\end{equation}\\
	\begin{equation}
	impact^{y_j}(m_i) = \sum_{j=1, j \neq ref}^{|Y|} \: p(y_j \: | \: x \leftarrow m_i = m_a) \: - \: p(y_j \: | \: x)
	\end{equation}\\
	\begin{equation}
	IMP(m_i) = \; \underset{m_a}{max} \; (impact^{y_{ref}}(m_i) \: + \: impact^{y_j}(m_i))
	\end{equation}\\
	
	
	où $y_{ref}$ est l'étiquette de référence (l'étiquette que l'on veut "expliquer"), $x$ est l'entrée, 
	$m_i$ est le mot que l'on retire de l'entrée $x$, $m_a$ est le mot par lequel on remplace $m_i$, 
	$y_j$ est une étiquette différente de $y_{ref}$, et $|Y|$ est le nombre d'étiquette. \\
	\\
	L'équation 9 pénalise l'augmentation de la probabilité de l'étiquette que l'on veut 
	expliquer lorsque l'on remplace $m_i$ par $m_a$. \\
	L'équation 10 encourage l'augmentation des probabilités des autres étiquettes lorsque l'on remplace $m_i$ par $m_a$.\\
	Pour calculer l'importance d'un mot, on additionne ces deux impacts.
	Cette formule suit notre intuition de base et également celle de Robnik-Sikonja et Kononenko \cite{ref8}. Nous avons rajouté la prise en compte de l'impact sur  
	les probabilités des autres étiquettes.\\
	Nous cherchons ici le mot de remplacement $m_a$ qui maximise cette mesure d'importance, pour trouver le meilleur exemple 
	adversarial pour l'entrée $x$. Ce que l'on cherche à faire est donc de comparer l'entrée $x$ avec le meilleur exemple 
	adversarial, caractérisé par le remplacement de $m_i$ par $m_a$.\\
	\\
	Cette méthode permet donc d'avoir une explication contrastée, puisque l'on compare notre entrée de base avec des entrées créées artificiellement en remplacant un mot par un autre. 
	De plus, elle est également sélective puisque l'on sélectionne les $\alpha$ mots dans la prémisse et les $\beta$ mots dans l'hypothèse ayant le plus contribué à l'étiquette $y_{ref}$. 
	Enfin, le programme surligne les mots les plus importants pour chaque étiquette, ce qui permet d'avoir une visualisation pour faciliter la compréhension. Nous respectons donc les trois 
	règles d'une bonne explication citées dans la section 4.2.
	
	\begin{figure}[h]
		\center
		\includegraphics[width=10cm, height=7cm, keepaspectratio]{stuck.png}
		\caption{\bf{Exemple d'une explication fournie par BAXI sur le système 2. Les mots surlignés en bleu ont contribué à la neutralité, 
		les mots surlignés en jaune ont contribué à l'inférence, et 
		les mots surlignés en rouge ont contribué à la contradiction.}}
	\end{figure}
		
\section{Cadre expérimental} % SECTION 7
	\subsection{Corpus SNLI et représentation de mots} %FINI
		Nous utilisons les corpus SNLI composés d'un fichier d'entraînement, de validation et de test \cite{ref}. 
		Ces corpus ont été réalisé par cinq annotateurs à l'aide d'une image accompagnée d'un texte bref -la prémisse- présentant 
		la dite image. Les annotateurs ont alors écrit une phrase étant neutre par rapport à la prémisse, une autre 
		étant en contradiction et une autre phrase qui pouvait être déduite de la prémisse : ils ont donné une hypothèse 
		et une étiquette. Pour que le corpus ne soit pas trop subjectif, les annotateurs ont eu accès à quelques paires 
		prémisse/hypothèse sans étiquette. Chacun d'entre eux a donné une étiquette, celle ayant eu le plus de voix a été décidé comme 
		l'étiquette \textit{gold} de la paire observée. Ainsi, certaines paires n'ont pas d'étiquette car les annotateurs n'ont pas trouvé de 
		consensus : nous ne prennons pas en compte ce genre d'entrée.\\
		La table ci-dessous est un échantillon du corpus SNLI : \\ 
		\begin{table}[h]
		\leftskip -2cm
			{
				\includegraphics[width=16cm, height=13cm, keepaspectratio]{table_SNLI.png}
			}
				\caption[Echantillon du corpus de développement de SNLI]{\bf{Echantillon de 5 paires du corpus de développement de SNLI présentant à gauche la prémisse, 
					à droite l'hypothèse, et au centre les étiquettes des 5 annotateurs (C pour Contradiction, N pour \textit{Neutral} (neutre), et E pour \textit{Entailment} (inférence)) 
					avec en premier l'étiquette de l'auteur de la paire. L'étiquette en gras 
					est celle qui a eu le plus de voix, et est donc l'étiquette \textit{gold} de la paire prémisse/hypothèse.}}
		
		\end{table}
		\\
		Pour mesurer les taux de réussite des explications, nous utilisons un échantillon du corpus de test de SNLI 
		composé de 19 paires prémisse/hypothèse avec leur étiquette associée, que nous avons appelé Corpus Pour l'Interprétabilité (CPI). 
		Ce corpus contient des paires se trouvant dans le corpus explicatifs (voir section 7.3) 
		et ayant des mots de remplacement (voir section 3.2). La table ci-dessous montre les taux des 
		étiquettes présentes dans ce petit corpus : \\
		
		\begin{table}[h]
			\begin{center}
			\begin{tabular}{|c|c|c|}
			\hline
			\small{\% Inférence} & \small{\% Neutre} & \small{\% Contradiction} \\
			\hline
			36,84\% & 26,31\% & 36,84\% \\
			\hline
			\end{tabular}
			\caption{\bf{Taux des étiquettes présentes dans le CPI.}}
			\end{center}
		\end{table}\\
		\\
		\newpage
		Pour la représentation des mots, nous utilisons des \textit{words embeddings} pré-entrainés via GloVe.6B.100d.
		Pour les mots inconnus, c'est-à-dire les mots qui n'ont pas d'\textit{embedding} dans GloVe, nous utilisons des \textit{embeddings}
		initialisés au hasard. 
		
	\subsection{Corpus explicatif} % FINI
		La méthode habituelle pour ce genre d'expériences est de montrer à plusieurs personnes les paires 
		de phrases prémisse/hypothèse avec leurs explications associées pour connaître leur avis sur la 
		qualité de l'explication.\\
		Nous proposons une autre méthode qui permet d'évaluer automatiquement la qualité d'une explication : nous 
		avons montré un échantillon de 43 exemples issus du fichier de test de SNLI à six personnes pour 
		qu'elles puissent annoter ce qu'elles pensent être une explication correcte, c'est-à-dire les mots 
		de la prémisse et les mots de l'hypothèse qui conduisent au label associé.
		Nous avons récolté leurs réponses pour en faire un corpus d'explications de références, au format CSV.\\
		La figure ci-dessous est un échantillon de ce corpus d'explications : \\
		
		\begin{table}[h]
			\begin{center}
			\leftskip -2cm
			{	
				\begin{tabular}{|p{2cm}|p{5cm}|p{3cm}|p{2cm}|p{2.5cm}|}
				\hline
				\small{étiquette} & \small{Prémisse} & \small{Hypothèse} & \small{Mots explicatifs (prémisse)} & \small{Mots explicatifs (hypothèse)}\\
				\hline
				inférence & This church choir sings to the masses as they sing joyous songs from the book at a church. &  The church is filled with song. & sings 3 songs 11 church 17 & filled 3 song 5 church 1\\
				\hline
				contradiction & A man playing an electric guitar on stage. &  A man playing banjo on the floor. & man 1 guitar 5 stage 7 & man 1 banjo 3 floor 6\\
				\hline
				neutre & An old man with a package poses in front of an advertisement. & A man poses in front of an ad for beer. & & for 8 beer 9 \\
				\hline
				inférence & A blond-haired doctor and her African american assistant looking threw new medical manuals. & A doctor is looking at a book & doctor 2 looking 8 manuals 12 & doctor 1 looking 3 book 6\\
				\hline
				\end{tabular}
			}
			\caption{\bf{Echantillon du corpus explicatif. Chaque ligne correspond à une paire. La première colonne est l'étiquette de la paire, la deuxième est 
			la prémisse, la troisième est l'hypothèse, la quatrième est la liste des mots explicatifs dans la prémisse, et la dernière est la liste 
			des mots explicatifs dans l'hypothèse. Le nombre à droite de chaque mot explicatif est sa position dans sa phrase correspondante : certains mots pouvent 
			être présent plusieurs fois dans la phrase, tel que "church" dans la prémisse du premier exemple. Il faut donc les différencier. Le troisième exemple 
			n'a pas de mot explicatif dans la prémisse.}}
			\end{center}
		\end{table}
			
	\subsection{Mots de remplacement} % FINI
		Pour que BAXI marche bien, il faut remplacer les mots par d'autres mots pertinents pour créer 
		des exemples adversariaux. Pour cela, on a annoté 
		trois fichiers -un pour chaque étiquette- contenant 19 paires se trouvant dans le corpus explicatif. Dans ces 
		fichiers, on spécifie pour chaque mot de chaque paires, les mots pouvant le remplacer.\\
		\\
		L'idée pour l'annotation des mots de remplacement est la suivante : si l'étiquette que l'on veut expliquer 
		se trouve être l'étiquette gold de la paire prémisse/hypothèse, on cherche alors à remplacer les mots des 
		phrases par des mots pouvant baisser fortement la probabilité de l'étiquette. Par exemple, 
		si l'étiquette gold est inférence, on va chercher à basculer vers la neutralité ou la contradiction pour faire 
		baisser l'inférence. \\
		A l'inverse, si l'étiquette que l'on veut expliquer n'est pas l'étiquette gold, on cherche alors à remplacer 
		les mots de la phrase par des mots pouvant augmenter la probabilité de l'étiquette expliquée (et par la même occasion, 
		faire baisser la probabilité des autres étiquettes). Par exemple, si l'étiquette gold est neutre et que l'on veut 
		expliquer la contradiction, on cherche à créer une contradiction avec les mots de remplacement.\\
		\\
		\begin{table}[h]
			\begin{center}
			\begin{tabular}{|c|c|}
			\hline
			\small{Prémisse} & \small{Liste de mots remplaçants} \\
			\hline
			A & Another That This The \\
			\hline
			woman & man boy person guy dude \\
			\hline
			with & without and wearing selling \\
			\hline
			a & that the this those these \\
			\hline
			green & blue sad happy depressed \\
			\hline
			headscarf & happiness joy sadness depression \\
			\hline
			, & and also ; \\
			\hline
			blue & sad depressed red green \\
			\hline
			shirt & headscarf jacket glasses scarf shoes \\
			\hline
			and & also , ;  \\
			\hline
			a & that the this those these \\
			\hline
			very & small depressed sad few \\
			\hline
			big & young old small depressed sad \\
			\hline
			grin & sandwich headscarf shirt depression sadness \\
			\hline
			. & ! \\
			\hline
			\end{tabular}
			\end{center}
		\end{table}\\
		
		\begin{table}[h]
			\begin{center}
			\begin{tabular}{|c|c|}
			\hline
			\small{Hypothèse} & \small{Liste de mots remplaçants} \\
			\hline
			The & Another A That This \\
			\hline
			woman & man boy person guy dude \\
			\hline
			is & was were \\
			\hline
			very & little few not \\
			\hline
			happy & old young tall small depressed \\
			\hline
			. & ! \\
			\hline
			\end{tabular}
			\caption{\bf{Exemple d'une prémisse (table d'en haut) et d'une hypothèse (table d'en bas) avec une liste de mots pouvant remplacer 
			le mot correspondant. L'étiquette de cette paire est inférence, et on veut expliquer l'inférence. Il faut donc des mots de remplacement qui fassent baisser l'inférence.}}
			\end{center}
		\end{table}\\
		


			
	\subsection{Paramètres} %FINI
		Tous les \textit{embeddings} sont mis-à-jour par le réseau. Ils sont de dimension 100. La taille des \textit{batches} est de 16. \\
		Concernant les RNNs utilisés, il n'y a qu'une seule couche, la dimension des états cachés est de 100, et le \textit{dropout} est à 0,3.\\
		Pour l'utilisation de LIME, on a fixé le nombre de voisin de l'entrée à 500.
		
\section{Résultats} % SECTION 8
	Pour contextualiser, la table ci-dessous montre les taux de réussite des étiquettes prédites pour les trois systèmes décrits en section 5 
	sur le CPI :\\
	
	\begin{table}[h]
		\begin{center}
		\begin{tabular}{|c|c|c|c|c|}
		\hline
		\small{Système} & \small{\% Inférence} & \small{\% Neutre} & \small{\% Contradiction} & \small{\% Total} \\
		\hline
		1 & \bf{71,43}\% & 60,00\% & 42,86\% & \bf{57,89}\% \\
		\hline
		2 & 42,86\% & \bf{80,00}\% & \bf{57,14}\% & \bf{57,89}\% \\
		\hline
		3 & \bf{71,43}\% & 40,00\% & 42,86\% & 52,63\% \\
		\hline
		\end{tabular}
		\caption[Résultats des tests de la RTE pour le corpus de l'échantillon de test.]{\bf{Résultats des taux de réussite des étiquettes prédites sur le CPI.}}
		\end{center}
	\end{table}	
	
	
	La table ci-dessous illustre les différents résultats obtenus pour le taux de réussite des explications des trois systèmes décrits en 
	section 5 sur le CPI et sur le corpus explicatif. 
	Le taux de réussite des explications correctes est mesuré par le nombre de mots explicatifs correctement trouvés sur le nombre de mots explicatifs total (se trouvant 
	dans le corpus explicatif). \\
	
	\begin{table}[h]
		\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		\small{Système} & \small{Méthode} & \small{\% Inférence} & \small{\% Neutre} & \small{\% Contradiction} & \small{\% Total}\\
		\hline
		\multirow{2}{*}{1} & BAXI & \bf{61,70\%} & \bf{57,14\%} & \bf{51,61\%} & \bf{57,61\%} \\
			\cline{2-6}
			& LIME & 27,66\% & 28,57\% & 32,26\% & 29,35\% \\
		\hline
		\multirow{2}{*}{2} & BAXI & \bf{76,60\%} & \bf{57,14\%} & 54,84\% & \bf{66,30\%} \\
			\cline{2-6}
			& LIME & 55,32\% & 50,00\% & \bf{61,29\%} & 56,52\% \\
		\hline
		\multirow{2}{*}{3} & BAXI & \bf{61,70\%} & \bf{57,14\%} & 51,61\% & \bf{57,61\%} \\
			\cline{2-6}
			& LIME & 31,91\% & 35,71\% & \bf{61,29\%} & 42,39\% \\
		\hline
		\end{tabular}
		\caption[Résultats des mesures de BAXI et LIME]{\bf{Résultats des mesures de BAXI et LIME sur le CPI avec le corpus explicatif.}}
		\end{center}
	\end{table}
	
	/* mettre des graphiques */

\section{Discussion} % SECTION 9 FINI
	BAXI a cependant des limites que nous allons corriger dans de futurs travaux : en effet, BAXI remplace seulement un seul mot dans l'entrée, 
	ce qui est problématique face aux expressions polylexicales. Par exemple, si l'entrée contient l'expression \textit{"in front of"}, BAXI remplace le mot \textit{"in"} 
	pour faire une nouvelle entrée, puis le mot \textit{"front"}, et enfin le mot \textit{"of"}, car un seul mot peut être remplacé. Cela ne donne pas forcément de nouvelles entrées ayant du sens. BAXI est donc 
	actuellement en ordre 1, l'idéal serait qu'il soit en ordre $N$ pour remplacer $N$ mots.\\
	Une autre limite de la méthode est l'explication de la neutralité, car il est parfois difficile de l'expliquer avec des mots contribuant à la neutralité. Peut-être faudrait-il 
	expliquer ce qui est contre l'inférence et contre la contradiction dans une entrée étiquettée "neutre".\\
	Par ailleurs, nous avons noté à la main les mots pouvant remplacer chaque mot dans l'entrée. Cela peut se faire de manière automatique à l'aide d'outils tel que WordNet, cependant il faudrait alors reconnaitre les 
	expressions polylexicales, de plus cela deviendrait un exercice aussi difficile que l'inférence textuelle. Nous pensons qu'il est préférable de les annoter à la main.
	\\
	Concernant SNLI, le corpus contient des biais que BAXI arrive à retrouver, surtout concernant la neutralité : les annotateurs ont souvent pris l'idée que pour qu'une entrée soit neutre, 
	l'hypothèse doit rajouter un but \cite{ref23}. Ainsi, BAXI surligne le mot \textit{"for"} en faveur de la neutralité quand il le voit dans l'hypothèse. Il faudrait cependant avoir plus de données de test 
	pour être certain qu'il retrouve toujours ce genre de biais.\\
	Ces biais sont la conséquence de la façon dont SNLI a été construit (voir section 7.1 pour l'explication de la construction de SNLI). 
	De plus, il arrive que certains éléments de l'image présentée ne se trouvent pas dans la prémisse, ce qui engendre des étiquettes difficiles à prédire. La figure ci-dessous en est un exemple :\\

	\begin{figure}[h]
		\center
		\includegraphics[width=10cm, height=7cm, keepaspectratio]{splashing.png}
		\caption{\bf{Exemple d'une explication fournie par BAXI sur le système 2. Les mots surlignés en bleu ont contribué à la neutralité, 
		les mots surlignés en jaune ont contribué à l'inférence, et 
		les mots surlignés en rouge ont contribué à la contradiction.}}
	\end{figure}\\	
	Dans cet exemple, l'étiquette gold est "inférence", cependant il n'y a aucune mention d'éclaboussures d'eau dans la prémisse. Selon BAXI, 
	\textit{"splashing"} a donc contribué à la neutralité. Il a également contribué à l'inférence, puisqu'il est en présence de mots tels que \textit{"river"} et \textit{"water"}.\\
	\\
	En outre, lorsque les annotateurs choisissent une hypothèse et une étiquette, il faudrait qu'ils annotent aussi les mots ayant conduits à leur choix pour pouvoir directement avoir un corpus explicatif 
	pour mesurer la qualité des explications données. Nous pensons qu'en plus du taux de réussite des étiquettes prédites, il faut prendre en compte le taux de réussite des explications pour savoir si 
	un modèle est bon ou non. 

\section{Conclusion} % SECTION 10

\section{Remerciements} %SECTION 11 FINI
	J'adresse mes sincères remerciements à M. Benoit Favre pour ses conseils, son écoute, et 
	la confiance qu'il m'a accordée durant 
	ce projet, ce qui m'a permit de m'accomplir totalement dans mes missions.\\
	Je remercie également l'ensemble de l'équipe TALEP pour leur accueil et leur aide 
	lors des différentes réunions, ainsi que l'ensemble des 
	membres du jury pour avoir accepté la charge de la lecture critique et de l'évaluation de ce travail.\\
	Enfin, je remercie les annotateurs du corpus explicatif, Thomas Delfino, Franck Dary, Simone Fuscone, Sébastien Ratel, et Jérémy Auguste
	 pour m'avoir accordé un peu de temps lors de la création de ce corpus.
	  
\bibliographystyle{unsrt} % Le style est mis entre accolades.
\bibliography{biblio}

%\newpage
%\appendix
%\section{Annexes}
%\subsection*{Résultats expérimentaux}




\end{document}














