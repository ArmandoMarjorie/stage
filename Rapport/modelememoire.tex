

% Les packages utilise's ci-dessous le sont a` titre indicatif ;
% vous pouvez les changer a` votre convenance.

% Le type de document: article, rapport...
\documentclass[a4paper]{article}

% Mettre les diffe'rents packages et fonctions que l'on utilise
%\usepackage[english]{babel}
\usepackage[french]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage[nottoc, notlof, notlot]{tocbibind}
\usepackage{appendix}
\usepackage{comment}
\usepackage{multirow}


% Commenter l'une de ces deux lignes
%\RequirePackage[applemac]{inputenc}
\RequirePackage[latin1]{inputenc}
\usepackage[T1]{fontenc}

\begin{document}


%----------- A   C O M P L E T E R   P A R   L E S   A U T E U R S ------------


% Titre du rapport
\def\TitreRapport{
    Interprétation de prédictions de modèles pour l'inférence textuelle 
    en perturbant significativement les entrées
}

% Pre'nom et nom de l'autrice
\def\NomsAuteurs{
    Marjorie Armando
}

% Date du rapport (dans la me^me langue que le titre)
\def\DateRapport{
    1 juin 2018
}

% Nom des encadrants
\def\Encadrants{
    \textbf{Encadrant} \\
    Benoit Favre
}
% Nom du laboratoire
\def\Labo{
    Laboratoire d'Informatique et des Systèmes - LIS
}



% Re'sume' en franc,ais avec mots-cle's
\def\ResumeFrancais{
    La reconnaissance de l'inférence textuelle (Recognizing Textual Entailment : RTE) est au coeur de tous les aspects de 
    la compréhension de texte en traitement automatique du langage (TAL). 
    Le but de la RTE est de savoir automatiquement si une phrase, appelée l'hypothèse, est 
    déduite d'une autre phrase, appelée la prémisse.
    En utilisant des réseaux de neurones complexes, nous pouvons obtenir de bons taux de réussite pour la RTE. Cependant, 
    ces réseaux ne sont pas interprétables, ainsi nous n'avons pas la possibilité de savoir si le modèle s'est basé sur 
    de bonnes informations pour sa décision. 
    Dans ce travail, nous proposons la méthode Best Adversarial eXemple for Interpretability (BAXI), qui respecte les règles d'une "bonne" 
    explication pour rendre un modèle interprétable dans le cadre de la RTE, 
    avec l'utilisation du corpus SNLI \cite{ref}. Nous allons la comparer 
    à la méthode Local Interpretable Model-agnostic Explanations (LIME) \cite{ref2} qui permet 
    d'expliquer les prédictions de n'importe quel classifieur en apprenant localement un modèle interprétable dans le voisinage
    de l'entrée. Pour cela, nous avons extrait un échantillon du corpus de test de 
    SNLI, et demandé à six annotateurs de donner les mots expliquant l'étiquette associée à la paire prémisse/hypothèse. 
    Grâce à ce nouveau corpus, nous mesurons le taux de la qualité de l'explication fournit 
    par la méthode BAXI et la méthode LIME, sur plusieurs systèmes 
    que nous avons implémenté.
    \\[2mm]
    {\bf Mots-clés : } inférence textuelle, apprentissage automatique,\\traitement automatique des langues naturelles, 
    interprétabilité, LSTM
}


\thispagestyle{empty}
\begin{center}
\baselineskip=1.3\normalbaselineskip
{\bf\Large \TitreRapport}\\[8mm]
{\bf\large \NomsAuteurs}\\[1mm]
{\Labo}\\[4mm]
\Encadrants\\[10mm]

{\bf Résumé}
\end{center}

\ResumeFrancais\\[4mm]

\newpage
\tableofcontents
\newpage
\listoftables
\listoffigures
\newpage

%-------------------- T E X T E   D U   R A P P O R T -------------------------

\section{Introduction}
	\subsection{Contexte et motivations}
		Dans la pratique, il y a souvent un compromis entre le taux de réussite et l'interprétabilité du modèle. 
		Certains modèles tels que les arbres de décisions ou encore les modèle linéaires sont facilement interprétables, 
		et sont donc parfois utilisés à la place de modèles complexes tels que les réseaux de neurones profonds, 
		même si ceux-ci peuvent donner de meilleurs résultats.\\
		Pouvoir interpréter un modèle permettrait de le rendre utilisable dans des domaines où les décisions doivent être 
		mûrement réfléchies, telle que la médecine. Si un modèle propose de donner un certain traitement à un patient, 
		il faut que le modèle puisse donner de bonnes explications pour que le docteur l'approuve, car les conséquences pourraient 
		être catastrophique.\\
		\\
		Dans le cadre de la RTE, les modèles obtenant les meilleurs taux de réussite, décrit dans la section , sont non interprétables. 
		Le problème qui se pose avec l'utilisation de modèles non interprétables est qu'ils fournissent 
		uniquement les probabilités de chaque étiquette. Comment savoir, de manière humainement compréhensible, pourquoi le modèle a 
		associé une étiquette particulière à une certaine entrée ?\\
		\\
		Avoir un modèle digne de confiance permettrait au modèle d'être d'avantage utilisé : en effet, il a été observé, par exemple, que le fait de fournir des 
		explications augmente l'acceptation des recommandations de films \cite{ref2}. De plus, cela permettrait également au développeur de choisir 
		un modèle parmi ceux qu'il a implémenté, 
		car le taux de réussite n'est pas le seul critère à prendre en compte : un modèle peut fournir l'étiquette attendue 
		en se basant sur de mauvaises informations.

	\subsection{Tâches}
		Les concepts sémantiques d'inférence et de contradiction sont au coeur de tous les aspects de la compréhension de texte en TAL. 
		Ainsi, l'inférence textuelle en langage naturel (Natural Language Inference : NLI), caractérisant ces relations, est essentielle dans des tâches telles 
		que la recherche d'information, le raisonnement de bon sens, ou encore le système 
		de question/réponse.\\
		\\
		Soit une paire de phrase prémisse/hypothèse, la RTE se voit comme 
		objectif de détecter si la seconde phrase est en contradiction, se déduit ou bien est neutre par rapport à la 
		première phrase. Il y a donc trois étiquettes permettant d'illustrer la 
		relation entre la prémisse et l'hypothèse : \\
		\begin{itemize}\renewcommand{\labelitemi}{$\bullet$}
			\item Contradiction : l'hypothèse contredit la prémisse. Par exemple :\\
				\textit{
				Prémisse : "Le chat est entièrement blanc."\\ 
				Hypothèse : "Le chat est entièrement noir."}\\

			\item Neutre : l'hypothèse est possible dans le contexte de la prémisse. Par exemple :\\
				\textit{
				Prémisse : "Le chat dort sur la banquette."\\
				Hypothèse : "Le chat aime le chocolat."}\\

			\item Inférence : l'hypothèse est déduite de la prémisse. Par exemple :\\
				\textit{
				Prémisse : "Le chat aimerait manger la sourie."\\
				Hypothèse : "Le chat a faim."}\\
		\end{itemize} \\
		\\
		\\
		Dans ce travail, nous proposons d'interpréter les prédictions des modèles axés sur la RTE avec la méthode Best Adversarial eXemple for Interpretability (BAXI), expliquée en section 4. 
		Nous allons comparer ses performances avec 
		la méthode Local Interpretable Model-agnostic Explanations (LIME) de Ribeiro et al., expliquée dans la section 3.3, grâce à un corpus d'explications que nous avons créé à l'aide de six annotateurs. 
		Les résultats sont visibles en section 6.\\
		Nous allons tout d'abord lister nos hypothèses scientifiques, puis énoncer l'état de l'art pour la RTE ainsi que pour l'interprétabilité de prédictions de modèle en général. Puis, nous allons décrire notre méthode 
		ainsi que le cadre expérimental. Enfin, nous allons décrire nos résultats obtenus puis conclure sur les différentes perspectives.


		\begin{comment}
		Par ailleurs, on peut interpréter de manière globale en expliquant le comportement général du modèle, ou de manière locale en expliquant 
		pourquoi le modèle a choisi tel label. Notre objectif se place dans le cas de l'interprétation locale : le but est d'expliquer 
		pourquoi le modèle a choisi un label $y$ pour une entrée $x$.\\
		\\
		Dans ce travail, nous allons tout d'abord énumérer quelques méthodes existantes pour la RTE et pour l'interprétation de prédictions d'un modèle.
		Nous allons ensuite définir ce que nous devons attendre d'une "bonne" explication, puis spécifier les modèles que nous avons implémenté pour la RTE ainsi que les corpus utilisés. 
		Puis, nous allons expliciter notre technique pour l'interprétation d'une prédiction d'un modèle, et nous allons la comparer avec LIME grâce à une nouvelle méthode 
		mesurant le taux d'explications correctes. Enfin, nous allons conclure avec les différentes perspectives. 
		\end{comment}
		%\bigskip
		
\section{Hypothèse scientifique}

\section{Etat de l'art}
	\subsection{Dans le domaine de la RTE}
		Les premiers travaux sur la RTE ont été 
		formé sur de très petits ensembles de données avec des méthodes conventionnelles, 
		telles que les méthodes peu profondes \cite{ref9} ou encore
		les méthodes de logique naturelle \cite{ref10}.
		\\
		Ces dernières années, on dénote une nette amélioration pour la tâche de la RTE, en particulier 
		grâce à la publication du corpus SNLI (the Stanford Natural Language Inference) qui contient 
		570K paires de phrases annotées. Cela a permit d'entrainer des réseaux de neurones 
		complexes, puisqu'ils nécessitent une quantité relativement importante de données. En 
		effet, la recherche sur l'apprentissage automatique dans ce domaine été limitée 
		par le manque de corpus assez grand.\\
		\\
		On peut distinguer deux types d'approches parmis ces modèles : les modèles basés sur l'encodage 
		des phrases 
		et les modèles axés sur l'attention inter-phrases.\\
		Les premiers types de modèles encodent les phrases puis un classifieur (un  
		perceptron multi-couches) décide de la relation entre ces deux phrases encodées. Différents encodeurs ont été 
		proposés, tels que les Long-Short Term Memory (LSTM) \cite{ref}, les Gated Recurrent Unit 
		(GRU) \cite{ref11}, les réseaux de neurones à convolution (Convolutionnal Neural 
		Network : CNN) \cite{ref12}, les Bidirectionnal Long-Short Term Memory (BiLSTM) et ses 
		variantes \cite{ref13} \cite{ref14} \cite{ref15}, et des réseaux neuronaux plus 
		complexes \cite{ref16} \cite{ref17}.\\
		L'avantage de ces modèles est que les encodeurs transforment les phrases 
		en vecteur de taille fixe, ce qui peut aider à un large éventail de tâches 
		de transfert \cite{ref18}. Cependant, cette architecture ignore l'interaction 
		locale entre deux phrases, pourtant nécessaire \cite{ref10}.\\
		Les seconds modèles ont alors été proposé pour éviter ce problème.  
		Dans ce cadre, l'information d'inférence locale est collectée par le mécanisme d'attention 
		puis introduite dans des réseaux de neurones pour composer des vecteurs de taille fixe avant la 
		classification finale. Beaucoup suivent cette route. Parmis eux, Rocktäschel et al. \cite{ref19} 
		ont été les premiers à proposer des réseaux basés sur l'attention pour la RTE. 
		Chen et al. \cite{ref20} ont proposé un modèle d'inférence séquentielle amélioré qui est 
		l'un des meilleurs modèles à ce jour (88,6\% de taux de réussite).
		
	\subsection{Dans l'interprétabilité pour le TAL}
		Dans la pratique, il y a souvent un compromis entre le taux de réussite et l'interprétabilité du modèle.
		Certains modèles tels que les arbres de décisions ou encore les modèle linéaires sont facilement interprétables, 
		et sont donc parfois utilisés à la place de modèles complexes tels que les réseaux de neurones profonds,  
		même si ceux-ci peuvent donner de meilleurs résultats. Cependant, au cours de 
		ces dernières années, de grandes avancées 
		ont été effectuées pour tenter d'interpréter les modèles utilisés jusqu'alors comme des boîtes noires.\\
		\\
		Nous étudions dans ce travail la méthode LIME qui permet d'expliquer les prédictions de n'importe quel classifieur 
		ou regresseur. L'objectif global de LIME est d'identifier un modèle interprétable parmi le voisinage de l'entrée $x$.\\
		\\ 
		Tout d'abord, on distingue les features utilisées aux représentations interprétables des features. Par exemple, 
		les features sont les embeddings des mots et la représentation interprétable de ces features est un vecteur binaire 
		qui indique la présence ou l'absence des mots.\\
		\\
		LIME définit une explication par un modèle $g \in G$, où $G$ est la classe des modèles interprétables tels que 
		les modèles linéaires ou les arbres de décisions. Vu que les modèles interprétables n'ont pas tous la même difficulté 
		à être interprété, LIME définit $\Omega(g)$ qui est une mesure de la complexité d'interpréter $g$. En 
		prenant l'example des arbres de décisions, $\Omega(g)$ est la profondeur.\\
		On dénote par $f : \mathbb{R}^{d} \rightarrow \mathbb{R}$ le modèle utilisé comme une boîte noire. $f(x)$ est la 
		probabilité que l'entrée $x$ appartienne à une certaine classe.\\
		LIME va alors se baser sur la représentation interprétable des données en retirant un ou plusieurs mots au hasard. On dénote cette 
		nouvelle entrée par $z$. LIME définit la localité de $x$ avec $\pi_x(z)$ qui est une mesure de proximité entre $z$ et 
		$x$. C'est un noyau se basant sur la similarité cosinus.\\
		Enfin, on définit $\mathcal{L}(f, g, \pi_x)$ qui est une mesure pour savoir à combien $g$ est infidèle à $f$ dans la localité 
		défini par $\pi_x$. Pour préserver à la fois l'interprétabilité et la fidèlité locale, LIME minimise $\mathcal{L}(f, g, \pi_x)$ 
		avec $\Omega(g)$ assez petit pour être interprétable par les humains. L'explication de LIME est donc la suivante :
		\begin{equation}
		\mathcal{E}(x) = \quad \underset{g \in G}{argmin}\quad \mathcal{L}(f, g, \pi_x) + \Omega(g)
		\end{equation}		
		Cette formule peut être utilisée par différents modèles $g \in G$, fonctions de 
		fidèlité $\mathcal{L}(f, g, \pi_x)$, et mesure de complexité $\Omega(g)$.\\
		\\
		LIME peut alors donner les $K$ mots les plus importants de l'entrée $x$ pour tout label.\\
		La figure ci-dessous est un exemple illustant le principe de LIME :
		
		\begin{figure}[h]
			\center
			\includegraphics[width=8cm, height=6cm, keepaspectratio]{lime_exemple.png}
			\caption{Exemple présentant l'intuition de LIME.}
		\end{figure}
		La décision de la boîte noire $f$, inconnu par LIME, est représentée par le fond bleu et rose. 
		La croix rouge en gras est l'entrée $x$ que l'on veut expliquer. LIME crée des entrées modifiées, 
		utilise $f$ pour avoir la probabilité de ces entrées pour le label $y$, et les pondère par leur proximité 
		par rapport à $x$ (les poids sont représentés par la taille). La droite pointillée est l'explication apprise qui est 
		localement fidèle.\\
		
	\subsection{Dans l'interprétabilité de la RTE}
		Ce n'est que très recemment que Silva et al. \cite{ref21} 
		
		/*PARLER ARTICLE LREC*/
		
		\begin{comment}
		D'autres méthodes d'interprétation existent, telle que DeepLIFT (Deep Learning Important FeaTures :
		DeepLIFT) \cite{ref9} pour l'apprentissage profond, 
		qui décompose la prédiction d'un 
		réseau de neurones sur une entrée spécifique en rétropropageant les contributions de tous les neurones du réseau à 
		chaque feature de l'entrée. DeepLIFT compare l'activation de chaque neurone à son "activation de référence" et 
		attribue des scores de contribution en fonction de la différence. L'activation de référence est choisi par 
		l'utilisateur.\\
		On peut également citer la méthode de propagation de pertinence par couche \cite{ref10} pour l'interprétation des 
		réseaux de neurones profonds, une méthode équivalente à DeepLIFT avec l'activation de référence de tous les neurones 
		fixée à 0. 
		\end{comment}
		
	
		
		\begin{comment}
		mais nous pouvons également citer la méthode SHapley Additive exPlanations (SHAP) \cite{ref8} : 
		elle explique la prédiction de n'importe quel modèle en utilisant les valeurs de Shapley, introduites dans la théorie des jeux en 1953. 
		Ces valeurs ont récemment été utilisées pour attribuer une mesure d'importance aux features \cite{ref5}. SHAP appartient à la classe des "méthodes d'attribution de features additives" : 
		elle attribue une valeur à chaque feature pour chaque prédiction (c'est-à-dire une attribution de feature). 
		Plus la valeur est haute, plus l'attribution de la feature à la prédiction spécifique est grande :  
		la somme de ces valeurs devrait donc être proche de la prédiction initiale du modèle.\\
		\\
		SHAP a rassemblé et unifié six méthodes d'attribution de features additives, dont LIME, et est la seule méthode qui respecte les trois propriétés suivantes :
		\begin{description}
			\item[- Précision locale] : les explications sont fidèles et véridiques au modèle.\\
			\item[- Feature manquante] : les features retirées n'ont aucun impact attribué aux prédictions du modèle.\\
			\item[- Cohérence] : les explications sont cohérentes avec l'intuition humaine. Techniquement, la cohérence indique que si un modèle change de sorte que la 
			contribution d'une entrée augmente ou reste la même indépendamment des autres entrées, l'attribution de cette entrée ne devrait pas diminuer.\\
		\end{description}\\
		\\
		Le calcul de ces valeurs est cependant très coûteux : il faut entraîner le modèle sur tous les sous-ensembles de features S inclu F, où F est l'ensemble de toutes les features. 
		Les valeurs Shapley attribuent une valeur d'importance à chaque feature, ce qui représente l'effet sur la prédiction du modèle d'inclure cette feature. 
		Pour cela, un modèle est entraîné avec la feature présente, et un autre modèle est entraîné avec la feature retirée. 
		Ensuite, les prédictions des deux modèles sont comparées sur l'entrée courante, c'est-à-dire qu'on calcule leur différence. 
		Comme l'effet de la supression d'une feature dépend d'autres features du modèle, les différences précédentes sont 
		calculées pour tous les sous-ensembles possibles de features. Les valeurs de Shapley sont une moyenne pondérée de toutes 
		les différences possibles et sont utilisées comme attribution de feature.\\
		\\
		Un algorithme efficace a été mis au point, cependant il fonctionne uniquement sur les modèles basés sur les arbres.
		\end{comment}

\section{Définition d'interprétabilité}
	Il n'y a malheureusement pas de consensus concernant la définition d' "interprétabilité". 
	Miller définit cela comme étant le degré auquel un humain peut comprendre la cause d'une décision \cite{ref4}. 
	Un système a donc une meilleure interprétabilité qu'un autre si ses explications sont plus faciles à comprendre par un humain.
	\begin{figure}[h]
		\center
		\includegraphics[width=10cm, height=6cm, keepaspectratio]{xai-figure.png}
		\caption[Concept des intelligences artificielles explicables]{Concept des intelligences artificielles explicables (eXplainable Artificial Intelligences : XAI) \cite{ref22}.}
	\end{figure}	
	
	\subsection{Qu'est-ce-qu'une explication ?}
		La définition donnée par Miller est assez simple : une explication est une réponse à une question commençant par "pourquoi". 
		Une question commençant par "comment" peut être retournée en une question commençant par "pourquoi".
		Le terme "explication" désigne le processus social et cognitif d'expliquer, mais c'est également le produit de ces processus.
	\subsection{Qu'est-ce-qu'une "bonne" explication ?}
		La définition d'une bonne explication ne doit pas se baser sur l'intuition de l'auteur, mais plutôt sur des faits. 
		Miller résume ce qu'est une bonne explication \cite{ref5}, basée sur ce que les humains attendent d'une explication. 
		Grâce à cela, nous allons énoncer les types d'explications adéquats à notre projet. Cependant, 
		il ne faut pas oublier que les humains ont tendance à rejeter toutes explications allant à l'encontre de leur croyance.
		\paragraph*{Explication contrastée}
			C'est une explication qui doit être comparée. Les utilisateurs se demandent généralement pourquoi cette prédiction a 
			été faite et pas une autre, via la question "quelle aurait été la prédiction si cette entrée avait été changé 
			par une autre ?".\\
			Un docteur se demandant "pourquoi ce traitement ne marche pas sur ce patient ?" voudrait comparer les données de ce patient 
			à un autre patient ayant des caractéristiques similaires mais pour qui le traitement marche.\\
			La meilleure explication pour ce type d'explication est celle qui met en évidence les différences entre l'entrée traitée et 
			l'entrée de comparaison.\\
			L'entrée de comparaison peut être artificielle.
		\paragraph*{Explication sélective}
			C'est une explication qui doit être courte. Généralement, on peut expliquer un phénomène par plusieurs facteurs. 
			Il faut en donner peu, à savoir deux ou trois raisons, même si les explications
			peuvent être plus complexes que cela.
		\paragraph*{Explication sociale}
			Comme nous l'avons expliqué ci-dessus, une explication est un processus social, 
			c'est-à-dire qu'il faut prendre en compte les connaissances de la personne à qui l'on veut donner une explication. 
			Dans notre projet, nous partons du principe qu'une explication doit être comprise par tout le monde, 
			que ce soit par un expert du domaine de l'apprentissage automatique ou bien par quelqu'un qui n'en a jamais entendu parler.
	\begin{comment}
	\subsubsection{Explication anormale}
		Les explications "anormales" sont beaucoup appréciées, c'est-à-dire que si une cause rare a influencé la prédiction, 
		il faut la spécifier. Dans notre projet, une cause rare peut être un mot -ou un groupe de mot- de l'hypothèse qui ne peut pas 
		être mis en relation avec un mot -ou un groupe de mot- de la prémisse, mais qui a influencé la prédiction.
	\end{comment}
	
\section{Description de l'approche}
	\subsection{Systèmes implémentés}
		Nous avons implémenté 3 systèmes différents avec la librairie DyNet \cite{ref6} en C++. 
		Nous utilisons les LSTMs et les BiLSTMs.
		
	\subsubsection{Systèmes basés sur l'encodage des phrases}
		\paragraph*{Premier système}
		Le premier système passe la prémisse et l'hypothèse au LSTM pour avoir une représentation pour chacune de ces deux phrases.  
		On les concatène pour les envoyer ensuite à une couche de décision : 
		\begin{equation}
		y = softmax( W \times [ LSTM(\textit{prémisse}) ; LSTM(\textit{hypothèse}) ] + b )
		\end{equation}
		où $y$ est un vecteur contenant les probabilités de chaque label, $W$ est la matrice de poids, $LSTM(\textit{prémisse})$ et $LSTM(\textit{hypothèse})$ sont respectivement la représentation de 
		la prémisse et de l'hypothèse, $b$ est le biais, et [;] dénote la concaténation.
		\paragraph*{Deuxième système}
		Le deuxième système effectue le même mécanisme que le premier système pour avoir une représentation de la prémisse et de 
		l'hypothèse. On compare les deux représentations pour envoyer la comparaison à une couche de décision :
		\begin{equation}
		y = softmax( W \times ( LSTM(\textit{prémisse}) \times LSTM(\textit{hypothèse})^{T} ) + b )
		\end{equation}
		où $T$ dénote la transposée.
	\subsubsection{Système avec mécanisme d'attention}
		\paragraph*{Troisième système}
		Le troisième système est inspiré de la méthode KIM \cite{ref3}.\\ 
		On représente les mots de la prémisse et de l'hypothèse en les passant dans un BiLSTM : il utilise un LSTM forward pour 
		lire la phrase de gauche à droite, puis un LSTM backward pour lire la phrase dans l'autre sens. A chaque mot lu, un 
		état caché est généré par les deux LSTMs. Ces deux états cachés sont alors concaténés pour obtenir une 
		représentation du mot :\\
		$h_t = [h_t^{\rightarrow} ; h_t^{\leftarrow}]$, 
		où $h_t^{\rightarrow}$ est l'état caché généré par le LSTM forward à 
		l'instant t, $h_t^{\leftarrow}$ est celui généré par le LSTM backward à l'instant $t$, et $h_t$ est la 
		représentation du mot $t$.\\
		On dénote par $p^s$ (respectivement $h^s$) le vecteur de représentation des mots de la prémisse 
		(respectivement de l'hypothèse).\\
		Nous construisons ensuite une matrice d'alignement comme suit :
		\begin{equation}
		e_{ij} = (p_i^s)^T h_j^s
		\end{equation}
		où $p_i^s$ est la représentation du i\up{ème} mot de la prémisse, $h_j^s$ est celle du j\up{ième} mot de l'hypothèse.\\
		Avec cette matrice, nous pouvons alors construire les vecteurs de contexte $p^c$ et $h^c$ suivants pour la prémisse et l'hypothèse :
		\begin{equation}
		\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{N} exp(e_{ik})} , p_i^c = \sum_{j=1}^{N} \alpha_{ij} h_j^s   
		\end{equation}
		\begin{equation}
		\beta_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{M} exp(e_{kj})} , h_j^c = \sum_{i=1}^{M} \beta_{ij} p_i^s   
		\end{equation}
		où $M$ est la longueur de la prémisse, $N$ est la longueur de l'hypothèse, $\alpha \in \mathbb{R}^{M \times N}$ est un $softmax(e)$ 
		sur la prémisse, et $\beta \in \mathbb{R}^{M \times N}$ est un $softmax(e)$ sur l'hypothèse. Ceci permet à la prémisse de voir le contexte de l'hypothèse 
		et vice-versa.\\
		Avec ces nouvelles représentations pour les mots, on effectue du mean-pooling :
		\begin{equation}
		pool_p = \frac{\sum_{i=1}^{N} p_i^c}{N} , pool_h = \frac{\sum_{i=1}^{M} h_i^c}{M}
		\end{equation}
		puis on effectue une concaténation du mean-pooling de la prémisse et de l'hypothèse pour l'envoyer à une couche de décision :
		\begin{equation}
		y = softmax( W \times [ pool_p ; pool_h ] + b )
		\end{equation}		
		\\
		Les performances de ces systèmes et autres détails techniques 
		sont décrits en annexe. 
		Les schémas de ces systèmes figurent également en annexe.
	\subsection{Méthode BAXI}	
		Dans cette section nous allons décrire notre technique pour interpreter une prédiction d'une entrée. Nous voulons donner des explications pour chaque label 
		en donnant les trois mots les plus importants dans la prémisse et les trois mots les plus importants dans l'hypothèse. On veut donc calculer l'importance de chaque mot. 
		L'importance d'un mot correspond à l'importance de sa contribution pour le label $y$.\\
		\\
		Notre intuition est la suivante : 
		On veut donner une explication pour le label $y$ et l'entrée $x$ composée de la prémisse et de l'hypothèse. 
		On retire alors un mot et on demande à notre modèle de nous donner les probabilités de chaque label avec cette nouvelle entrée. Si la probabilité 
		du label $y$ a baissé, alors le mot était important : cela veut dire que le mot avait contribué au label $y$. A l'inverse, si elle a augmenté, le mot n'avait donc pas contribué au label $y$. 
		De plus, si la probabilité des autres labels a augmenté, alors le mot a d'autant plus d'importance : en le retirant, l'entrée $x$ a basculé vers un autre label.\\
		\\
		Pour résumé, lorsque l'on retire $m_i$, nous pénalisons l'augmentation de la probabilité du label que l'on veut expliquer, et nous encourageons l'augmentation des probabilités des autres labels.
		\\
		L'importance d'un mot, que nous notons $IMP^{total}$ pour "importance", est une fonction comme suit :
		\begin{equation}
		IMP^{y_j}(m_i) = \sum_{j=1, j \neq ref}^{|Y|} \log( p(y_j | x \setminus m_i) ) - \log( p(y_j | x) ) 
		\end{equation}
		\begin{equation}
		IMP^{y_{ref}}(m_i) = -\log( p(y_{ref} | x \setminus m_i) ) + \log( p(y_{ref} | x) )
		\end{equation}
		\begin{equation}
		IMP^{total}(m_i) = IMP^{y_{ref}}(m_i) + IMP^{y_j}(m_i) 
		\end{equation}\\
		où $y_{ref}$ est le label de référence (le label que l'on veut "expliquer"), $x$ est l'entrée, $m_i$ est le mot que l'on retire de l'entrée $x$, 
		$y_j$ est un label différent de $y_{ref}$, et $|Y|$ est le nombre de label. Les probabilités sont exprimées en log-probabilités pour des soucis de précision et d'optimisation du code. On parle donc plutôt de score.\\
		Cette formule suit notre intuition de base et également celle de Robnik-Sikonja et Kononenko \cite{ref8}. Nous avons rajouté la prise en compte de l'impact sur les scores 
		des autres labels quand on retire le mot $m_i$.\\
		\\
		Un problème persiste tout de même : que se passe-t-il vraiment lorsque l'on retire un mot ? Nous ne pouvons pas simplement le supprimer de l'entrée, car la phrase n'aurait plus de sens 
		et le modèle n'est peut-être pas entraîné à faire face aux bruits. On ne peut pas vraiment le remplacer par un mot générique (le mot "UNK" pour "mot inconnu", par exemple) 
		car un modèle remplacant ce mot par un embedding à 0 et un modèle remplacant ce mot par un embedding particulier auront des réponses très différentes. Or on voudrait que notre méthode soit 
		réalisable par n'importe quel modèle. La solution est donc de remplacer le mot que l'on veut retirer par un autre :
		\begin{equation}
		p(y | x \setminus m_i) =  \sum_{s=1}^{|S|} p(y | x \leftarrow m_i = m_s) 
		\end{equation}\\
		où $|S|$ est le nombre de mot que l'on peut mettre à la place du mot $m_i$ et $m_s$ est le mot que l'on met à la place de $m_i$.\\
		%Les mots pouvant remplacer $m_i$ sont situés dans un fichier externe créé par nous-même. On a fixé à 5 le nombre de mots maximum pouvant remplacer $m_i$.
		Plus il y a de mots de remplacements, plus on a une meilleure estimation de l'importance du mot $m_i$, mais dans ce cas la complexité en temps augmente. Il faut donc trouver un compromis entre 
		l'estimation de l'importance d'un mot et le temps que met le programme pour donner une explication. Nous expliquons comment et par quoi nous avons remplacé les mots dans la section 6.1.\\
		\\
		Cette méthode permet donc d'avoir une explication contrastée, puisque l'on compare notre entrée de base avec des entrées créées artificiellement en remplacant un mot par un autre. 
		De plus, elle est également sélective puisque l'on sélectionne les K mots, respectivement dans la prémisse puis dans l'hypothèse, ayant le plus contribué au label $y_{ref}$. 
		Enfin, le programme surligne les mots les plus importants pour chaque label, ce qui permet d'avoir une visualisation pour faciliter la compréhension. Nous respectons donc les trois 
		règles d'une bonne explication citées dans la section 3.
		
		/* METTRE UN SCREEN D'UNE EXPLICATION */
		
\section{Cadre expérimental}
	\subsection{Corpus SNLI et représentation de mots} %FINI
		Nous utilisons les corpus SNLI composés d'un fichier d'entraînement, de validation et de test. 
		Ces corpus ont été réalisé par cinq annotateurs à l'aide d'une image accompagnée d'un texte bref -la prémisse- présentant 
		la dite image. Les annotateurs ont alors écrit une phrase étant neutre par rapport à la prémisse, une autre 
		étant en contradiction et une autre phrase qui pouvait être déduite de la prémisse : ils ont donné une hypothèse 
		et une étiquette. Pour que le corpus ne soit pas trop subjectif, les annotateurs ont eu accès à quelques paires 
		prémisse/hypothèse sans étiquette. Chacun d'entre eux a donné une étiquette, celle ayant eu le plus de voix a été décidé comme 
		l'étiquette gold de la paire observée. Ainsi, certaines paires n'ont pas d'étiquette car les annotateurs n'ont pas trouvé de 
		consensus : nous ne prennons pas en compte ce genre d'entrée.\\
		La table ci-dessous est un échantillon du corpus SNLI : \\ 
		\begin{table}[h]
			\center
			\includegraphics[width=13cm, height=12cm, keepaspectratio]{table_SNLI.png}
			\caption[Echantillon du corpus de développement de SNLI]{\bf{Echantillon de 5 paires du corpus de développement de SNLI présentant à gauche la prémisse, 
				à droite l'hypothèse, et au centre les étiquettes des 5 annotateurs (C pour Contradiction, N pour Neutral, et E pour Entailment). L'étiquette en gras 
				est celle qui a eu le plus de voix, et est donc l'étiquette gold de la paire prémisse/hypothèse.}}
		\end{table}\\
		\\
		Pour la représentation des mots, nous utilisons des \textit{words embeddings} pré-entrainés de dimension 100 via GloVe.6B.100d.
		Pour les mots inconnus, c'est-à-dire les mots qui n'ont pas d'\textit{embedding} dans GloVe, nous utilisons des \textit{embeddings}
		initialisés au hasard. Tous les \textit{embeddings} sont mis-à-jour par le réseau.
		
	\subsection{Mots de remplacement}
		Nous pensons que nous pouvons choisir automatiquement, pour chaque mot $m_i$ d'une phrase, les mots pouvant les 
		remplacer (avec des outils tel que WordNet, par exemple). Cependant, nous avons voulu les choisir 
		nous-même pour assurer la qualité des mots et essayer d'avoir les meilleurs exemples de comparaisons 
		possible. Nous avons donc annoté trois fichiers suivant le label que l'on veut "expliquer". Lorsque 
		l'on parle "d'expliquer un label", cela signifie que l'on donne les $K$ mots -dans la prémisse et dans 
		l'hypothèse- qui ont le plus contribué au label que l'on veut expliquer. \\
		\\
		L'idée pour l'annotation des mots de remplacement est la suivante : si le label que l'on veut expliquer 
		se trouve être le label gold de la paire prémisse/hypothèse, on cherche alors à remplacer les mots des 
		phrases par des mots pouvant baisser fortement le score du label. \\
		A l'inverse, si le label que l'on veut expliquer n'est pas le label gold, on cherche alors à remplacer 
		les mots de la phrase par des mots pouvant augmenter le score du label expliqué (et par la même occasion, 
		faire baisser le score des autres labels). \\
		
		/* FAIRE TABLE EXEMPLE DES MOTS DE REMPLACEMENT */
	\subsection{Corpus d'explications}
		 
	\subsection{Métrique}
		La méthode habituelle pour ce genre d'expériences est de montrer à plusieurs personnes les paires 
		de phrases prémisse/hypothèse avec leurs explications associées pour connaître leur avis sur la 
		qualité de l'explication. \\
		Nous proposons une autre méthode qui permet d'évaluer automatiquement la qualité d'une explication : nous 
		avons montré un échantillon de 43 exemples issus du fichier de test de SNLI à 5 personnes pour 
		qu'elles puissent annoter ce qu'elles pensent être une explication correcte, c'est-à-dire les mots 
		de la prémisse et les mots de l'hypothèse qui conduisent au label associé.
		Nous avons récolté leurs réponses pour en faire un fichier d'explications de références. \\
		/* A FINIR */
		
	\subsection{Paramètres}
		
\section{Résultats}

	Acc dans l'échantillon de test = 57.8947 \% pour les sys 1 et 2
	Les tables ci-dessous illustrent les différents résultats obtenus : \\
	
	\begin{table}[h]
	\begin{tabular}{|c|c|c|c|c|c|}
	\hline
	\small{Système} & \small{Méthode} & \small{\% Inférence} & \small{\% Neutre} & \small{\% Contradiction} & \small{\% Total}\\
	\hline
	\multirow{2}{*}{1} & BAXI & 57,45\% & 50,00\% & 58,06\% & 56,52\% \\
		\cline{2-6}
		& LIME & test & test & test & test \\
	\hline
	\multirow{2}{*}{2} & BAXI & 74,47\% & 71,43\% & 51,61\% & 66,30\% \\
		\cline{2-6}
		& LIME & test & test & test & test \\
	\hline
	\multirow{2}{*}{3} & BAXI & test\% & test\% & test\% & test\% \\
		\cline{2-6}
		& LIME & test & test & test & test \\
	\hline
	\end{tabular}
	\caption[Résultats des mesures de BAXI et LIME]{Résultats des mesures de BAXI et LIME avec nos 3 systèmes./*expliquer 
	quand on aura tous les résultats}
\end{table}
	

\section{Discussion}

\section{Conclusion}

\section{Remerciements}
 
\newpage 
\bibliographystyle{unsrt} % Le style est mis entre accolades.
\bibliography{biblio}

\newpage
\appendix
\section{Annexes}
\subsection*{Résultats expérimentaux}\\
Pour la représentation des mots, nous utilisons des words embeddings pré-entrainés de dimension 100 via GloVe.6B.100d.
Pour les mots inconnus, c'est-à-dire les mots qui n'ont pas d'embedding dans GloVe, nous utilisons des embeddings
initialisés au hasard. Tous les embeddings sont mis-à-jour par le réseau.\\
La taille des batches est de 16 et le dropout est à 0,3.\\
Concernant les RNNs utilisés, il n'y a qu'une seule couche et la dimension des états cachés est de 100.\\

\hspace{-5cm}
\begin{table}[h]
	\begin{tabular}{|c|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
	\hline
	\small{Système} & \small{Taux de réussite Contradiction} & \small{Taux de réussite Inférence} & \small{Taux de réussite Neutre} & \small{Taux de réussite Dev} \\
	\hline
	1 & 67,33\% & 73,90\% & 68,38\% & 69,89\% \\
	\hline
	2 & 78,55\% & 87,98\% & 74,99\% & 80,57\% \\
	\hline
	3 & 68,73\% & 74,92\% & 69,55\% & 71,09\% \\
	\hline
	\end{tabular}
	\caption{Résultats des tests de la RTE pour le fichier de validation.}
\end{table}

\begin{table}[h]
	\begin{tabular}{|c|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
	\hline
	\small{Système} & \small{Taux de réussite Contradiction} & \small{Taux de réussite Inférence} & \small{Taux de réussite Neutre} & \small{Taux de réussite Test} \\
	\hline
	1 & test\% & test\% & test\% & test\% \\
	\hline
	2 & 77,70\% & 86,10\% & 76,14\% & 80,07\% \\
	\hline
	3 & test\% & test\% & test\% & test\% \\
	\hline
	\end{tabular}
	\caption{Résultats des tests de la RTE pour le fichier de test.}
\end{table}

\end{document}














