

% Les packages utilise's ci-dessous le sont a` titre indicatif ;
% vous pouvez les changer a` votre convenance.

% Le type de document: article, rapport...
\documentclass[a4paper]{article}

% Mettre les diffe'rents packages et fonctions que l'on utilise
%\usepackage[english]{babel}
\usepackage[french]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphics,color}
\usepackage[nottoc, notlof, notlot]{tocbibind}
\usepackage{appendix}
\usepackage{comment}


% Commenter l'une de ces deux lignes
%\RequirePackage[applemac]{inputenc}
\RequirePackage[latin1]{inputenc}
\usepackage[T1]{fontenc}

\begin{document}


%----------- A   C O M P L E T E R   P A R   L E S   A U T E U R S ------------


% Titre du rapport
\def\TitreRapport{
    Interprétation de prédictions de modèles pour l'inférence textuelle 
    en perturbant significativement les entrées
}

% Pre'nom et nom dde l'auteur
\def\NomsAuteurs{
    Marjorie Armando
}

% Date du rapport (dans la me^me langue que le titre)
\def\DateRapport{
    1 juin 2018
}

% Nom des encadrants
\def\Encadrants{
    \textbf{Encadrant} \\
    Benoit Favre
}
% Nom du laboratoire
\def\Labo{
    Laboratoire d'Informatique et des Systèmes - LIS
}



% Re'sume' en franc,ais avec mots-cle's
\def\ResumeFrancais{
    La reconnaissance de l'inférence textuelle (Recognizing Textual Entailment : RTE) est un domaine assez récent en traitement
    automatique du langage (TAL). Le but de la RTE est de savoir automatiquement si une phrase, appelée l'hypothèse, est 
    déduite d'une autre phrase, appelée la prémisse. Pour résoudre cela, on utilise des systèmes issus de l'apprentissage 
    automatique.\\
    Le problème qui se pose avec l'utilisation de ces systèmes est que le modèle nous 
    fournit uniquement les probabilités de chaque label. Ainsi, aucune information supplémentaire n'est
    donnée : comment savoir, de manière humainement compréhensible, pourquoi le modèle a associé un label 
    particulier à une paire de phrases donnée ?\\Pouvoir répondre à cette question permettrait de rendre un modèle utilisable 
    dans des domaines où les décisions doivent être mûrement réfléchies telle que la médecine, car malgré l'expansion des
    réseaux de neurones, ceux-ci restent des boîtes noires et il est donc difficile de leur faire confiance sans avoir
    d'explications en retour.\\
    Dans ce travail, nous proposons une méthode respectant les règles d'une "bonne" explication 
    pour rendre un modèle interprétable dans le cadre de la RTE avec l'utilisation du corpus SNLI \cite{ref}.
    Nous allons la comparer avec la méthode Local Interpretable Model-agnostic Explanations (LIME) \cite{ref2} qui permet
    d'expliquer les prédictions de n'importe quel classifieur en apprenant localement un modèle interprétable dans le voisinage
    de l'entrée.  
    \\[2mm]
    {\bf Mots-clés : } inférence textuelle, apprentissage automatique,\\traitement automatique des langues naturelles, 
    interprétabilité, LSTM
}


\thispagestyle{empty}
\begin{center}
\baselineskip=1.3\normalbaselineskip
{\bf\Large \TitreRapport}\\[8mm]
{\bf\large \NomsAuteurs}\\[1mm]
{\Labo}\\[4mm]
\Encadrants\\[10mm]

{\bf Résumé}
\end{center}

\ResumeFrancais\\[4mm]

\newpage

%-------------------- T E X T E   D U   R A P P O R T -------------------------

\section{Introduction}
	\subsection{Contexte de l'étude}
		Le traitement automatique du langage naturel ou de la langue naturelle (TALN) ou des langues (TAL) est un domaine 
		pluridisciplinaire, qui fait collaborer l'intelligence artificielle, l'informatique théorique, la logique, la linguistique ou 
		encore les statistiques en vue de modéliser et de reproduire à l'aide de machines, la capacité humaine à produire et à 
		comprendre les énoncés linguistiques dans des buts de communication. \\
		\\
		Dans le domaine du TAL, nous retrouvons plusieurs niveaux d'analyses linguistiques pour représenter au mieux les langues 
		naturelles par les machines. On peut par exemple citer l'analyse lexicale qui permet d'identifier quels sont et où sont les 
		mots, ou encore l'analyse sémantique qui permet de comprendre le sens des mots.\\
		\\
		La RTE permet d'apporter des méthodes pour l'analyse lexicale et sémantique. Ceci peut permettre le développement de diverses 
		applications telles que la recherche d'information ou encore le système de question/réponse.\\
		\\
		Le but de la RTE est de savoir automatiquement si, à partir de deux phrases, on peut en déduire la deuxième de la première. 
		La première phrase est appelée la prémisse, et la seconde l'hypothèse. Trois étiquettes permettent d'illustrer la 
		relation entre la prémisse et l'hypothèse : 
		\begin{description}
			\item[- Contradiction] : l'hypothèse contredit la prémisse.\\
				\textit{Exemple :\\
				Prémisse : "Le chat est entièrement blanc."\\ 
				Hypothèse : "Le chat est entièrement noir."}

			\item[- Neutre] : l'hypothèse est possible dans le contexte de la prémisse.\\
				\textit{Exemple :\\
				Prémisse : "Le chat dort sur la banquette."\\
				Hypothèse : "Le chat aime le chocolat."}

			\item[- Inférence] : l'hypothèse est déduite de la prémisse.\\
				\textit{Exemple :\\
				Prémisse : "Le chat aimerait manger la sourie."\\
				Hypothèse : "Le chat a faim."}
		\end{description} \\
		\\
		Pour résoudre cela, on utilise des systèmes issus de l'apprentissage automatique. Nous utilisons les réseaux de neurones 
		récurrents (Recurrent Neural Network : RNN) dans ce projet.\\

	\subsection{Problématique}
		Le problème qui se pose avec l'utilisation de système issue de l'apprentissage automatique est que le modèle nous fournit 
		uniquement les probabilités de chaque label. Comment savoir, de manière humainement compréhensible, pourquoi le modèle a 
		associé un label particulier à une paire de phrases donnée ?\\
		\\
		Pouvoir répondre à cette question permettrait de rendre un modèle utilisable dans des domaines où les décisions doivent être 
		mûrement réfléchies, telle que la médecine. Si un modèle propose de donner un certain traitement à un patient, 
		il faut que le modèle puisse donner de bonnes explications pour que le docteur l'approuve, car les conséquences pourraient 
		être catastrophique. \\
		\\
		Ainsi, un modèle doit être digne de confiance. Cette confiance donnée par les humains à un système dépend des explications 
		données. Nous verrons dans la partie "Interprétabilité" quels sont les critères d'une "bonne" explication. De plus, si un système est 
		jugé digne de confiance, il sera davantage utilisé : en effet, il a été observé, par exemple, que le fait de fournir des 
		explications augmente l'acceptation des recommandations de films \cite{ref2}. \\
		\\
		Pouvoir donner une interprétation du système peut donc permettre aux utilisateurs d'adopter un modèle. 
		Mais cela peut également aider le développeur lors du choix d'un modèle parmi ceux qu'il a implémenté, 
		car le taux de réussite n'est pas le seul critère à prendre en compte : un modèle peut fournir le label attendu 
		en se basant sur de mauvaises features.\\
		\\
		Dans ce rapport, nous allons tout d'abord énumérer quelques méthodes existantes pour l'interprétation de prédictions d'un modèle.
		Nous allons ensuite spécifier les modèles que nous avons implémenté pour la RTE, puis nous allons définir ce que nous devons 
		attendre d'une "bonne" explication. Puis, nous allons expliciter notre technique pour l'interprétation d'une prédiction d'un modèle, et 
		nous allons la comparer avec LIME. Enfin, nous allons conclure avec les différentes perspectives. 
		
		%\bigskip

\section{Etat de l'art}
	% PARLER DE SNLI, AVANT IL N'Y AVAIT PAS DE LARGE DATASET
	% PARLER DE LIME 
	\subsection{Dans le domaine de la RTE} 
		Avant la publication du corpus SNLI \cite{ref}, la recherche sur l'apprentissage automatique dans ce domaine été limitée 
		par le manque de corpus assez grand. 
		Depuis, les plus grands taux de réussite ont atteint 89,3\% \cite{ref7}.
	\subsection{Dans l'interprétabilité}
		Dans la pratique, il y a souvent un compromis entre le taux de réussite et l'interprétabilité du modèle.
		Certains modèles tels que les arbres de décisions ou encore les modèle linéaires sont facilement interprétables, 
		et sont donc parfois utilisés à la place de modèles complexes tels que les réseaux de neurones profonds,  
		même si ceux-ci peuvent donner de meilleurs résultats. Cependant, au cours de 
		ces dernières années, de grandes avancées 
		ont été effectuées pour tenter d'interpréter les modèles utilisés jusqu'alors comme des boîtes noires.\\
		\\
		Nous étudions dans ce rapport la méthode LIME, /* EXPLIQUER LIME */
		\begin{comment}
		mais nous pouvons également citer la méthode SHapley Additive exPlanations (SHAP) \cite{ref8} : 
		elle explique la prédiction de n'importe quel modèle en utilisant les valeurs de Shapley, introduites dans la théorie des jeux en 1953. 
		Ces valeurs ont récemment été utilisées pour attribuer une mesure d'importance aux features \cite{ref5}. SHAP appartient à la classe des "méthodes d'attribution de features additives" : 
		elle attribue une valeur à chaque feature pour chaque prédiction (c'est-à-dire une attribution de feature). 
		Plus la valeur est haute, plus l'attribution de la feature à la prédiction spécifique est grande :  
		la somme de ces valeurs devrait donc être proche de la prédiction initiale du modèle.\\
		\\
		SHAP a rassemblé et unifié six méthodes d'attribution de features additives, dont LIME, et est la seule méthode qui respecte les trois propriétés suivantes :
		\begin{description}
			\item[- Précision locale] : les explications sont fidèles et véridiques au modèle.\\
			\item[- Feature manquante] : les features retirées n'ont aucun impact attribué aux prédictions du modèle.\\
			\item[- Cohérence] : les explications sont cohérentes avec l'intuition humaine. Techniquement, la cohérence indique que si un modèle change de sorte que la 
			contribution d'une entrée augmente ou reste la même indépendamment des autres entrées, l'attribution de cette entrée ne devrait pas diminuer.\\
		\end{description}\\
		\\
		Le calcul de ces valeurs est cependant très coûteux : il faut entraîner le modèle sur tous les sous-ensembles de features S inclu F, où F est l'ensemble de toutes les features. 
		Les valeurs Shapley attribuent une valeur d'importance à chaque feature, ce qui représente l'effet sur la prédiction du modèle d'inclure cette feature. 
		Pour cela, un modèle est entraîné avec la feature présente, et un autre modèle est entraîné avec la feature retirée. 
		Ensuite, les prédictions des deux modèles sont comparées sur l'entrée courante, c'est-à-dire qu'on calcule leur différence. 
		Comme l'effet de la supression d'une feature dépend d'autres features du modèle, les différences précédentes sont 
		calculées pour tous les sous-ensembles possibles de features. Les valeurs de Shapley sont une moyenne pondérée de toutes 
		les différences possibles et sont utilisées comme attribution de feature.\\
		\\
		Un algorithme efficace a été mis au point, cependant il fonctionne uniquement sur les modèles basés sur les arbres.
		\end{comment}

\section{Réseaux de neurones récurrents}
	\subsection{LSTM et Bi-LSTM}
		Dans ce travail, nous utilisons les réseaux de neurones récurrents, 
		et plus particulièrement les LSTMs (Long Short-Term Memory : LSTM) et les Bi-LSTMs (Bidirectionnal 
		Long Short-Term Memory : Bi-LSTM).\\ 
		Ils permettent de relier des informations vues antérieurement à la tâche 
		courante. On leur donne des informations à chaque instant $t$, créant ainsi une 
		cellule.\\
		\begin{figure}[h]
			\center
			\includegraphics{lstm_schema_.png}
			\caption{Schéma d'un LSTM contenant trois cellules.}
		\end{figure}\\
		Ce sont des RNNs particuliers capable d'apprendre une dépendance 
		très éloignée à notre tâche courante : ils ont la capacité de supprimer ou 
		d'ajouter des informations à l'état de la cellule, régulé par des portes 
		qui décident s'il faut laisser passer de l'information.\\
		Ces réseaux de neurones sont utilisés ici en tant qu'encodeur : on fournit en entrée, à chaque instant $t$, 
		l'embedding du t\up{ième} mot de la phrase traitée, à savoir la prémisse ou l'hypothèse. 
		Le réseau calcule alors à chaque instant $t$ un état caché $h_t$ comme suit :\\
		\begin{equation}
		i_t = \sigma(W_i x_t + U_i h_{t-1} + b_i)
		\end{equation}
		\begin{equation}
		f_t = \sigma(W_f x_t + U_f h_{t-1} + b_f)
		\end{equation}
		\begin{equation}
		u_t = \tanh(W_u x_t + U_u h_{t-1} + b_u)
		\end{equation}
		\begin{equation}
		o_t = \sigma(W_o x_t + U_o h_{t-1} + b_o)
		\end{equation}
		\begin{equation}
		c_t = f_t \odot c_{t-1} + i_t \odot u_t
		\end{equation}
		\begin{equation}
		h_t = o_t \odot \tanh(c_t)
		\end{equation}
		où $\sigma$ est la fonction sigmoid, $\odot$ est le produit de Hadamard entre deux vecteurs, 
		$W_i$, $W_f$, $W_u$, $W_o \in \mathbb{R}^{D \times D_e}$, 
		$U_i$, $U_f$, $U_u$, $U_o \in \mathbb{R}^{D \times D}$ et 
		$b_i$, $b_f$, $b_u$, $b_o \in \mathbb{R}^{D}$ sont des paramètres mis-à-jour par le réseau. $D$ est la dimension 
		des états cachés et $D_e$ est la dimension des embeddings.

	\subsection{Systèmes utilisés}
		Nous avons implémenté 3 systèmes différents avec la librairie DyNet \cite{ref6} en C++ :\\
		\\
		Le premier système passe la prémisse et l'hypothèse au LSTM pour avoir une représentation pour chacune de ces deux phrases.  
		On les concatène pour les envoyer ensuite à une couche de décision : 
		\begin{equation}
		y = softmax( W \times [ LSTM(\textit{prémisse}) ; LSTM(\textit{hypothèse}) ] + b )
		\end{equation}
		où $y$ est un vecteur contenant les probabilités de chaque label, $W$ est la matrice de poids, $LSTM(\textit{prémisse})$ et $LSTM(\textit{hypothèse})$ sont respectivement la représentation de 
		la prémisse et de l'hypothèse, $b$ est le biais, et [;] dénote la concaténation. \\
		\\
		\\
		Le deuxième système effectue le même mécanisme que le premier système pour avoir une représentation de la prémisse et de 
		l'hypothèse. On compare les deux représentations pour envoyer la comparaison à une couche de décision :
		\begin{equation}
		y = softmax( W \times ( LSTM(\textit{prémisse}) \times LSTM(\textit{hypothèse})^{T} ) + b )
		\end{equation}
		où $T$ dénote la transposée.\\
		\\
		\\
		Le troisième système est inspiré de la méthode KIM \cite{ref3}.\\ 
		On représente les mots de la prémisse et de l'hypothèse en les passant dans un Bi-LSTM : il utilise un LSTM forward pour 
		lire la phrase de gauche à droite, puis un LSTM backward pour lire la phrase dans l'autre sens. A chaque mot lu, un 
		état caché est généré par les deux LSTMs. Ces deux états cachés sont alors concaténés pour obtenir une 
		représentation du mot :\\
		$h_t = [\overset{\rightarrow}{h_t} ; \overset{\leftarrow}{h_t}]$, 
		où $\overset{\rightarrow}{h_t}$ est l'état caché généré par le LSTM forward à 
		l'instant t, $\overset{\leftarrow}{h_t}$ est celui généré par le LSTM backward à l'instant $t$, et $h_t$ est la 
		représentation du mot $t$.\\
		On dénote par $p^s$ (respectivement $h^s$) le vecteur de représentation des mots de la prémisse 
		(respectivement de l'hypothèse).\\
		Nous construisons ensuite une matrice d'alignement comme suit :
		\begin{equation}
		e_{ij} = (p_i^s)^T h_j^s
		\end{equation}
		où $p_i^s$ est la représentation du i\up{ème} mot de la prémisse, $h_j^s$ est celle du j\up{ième} mot de l'hypothèse.\\
		Avec cette matrice, nous pouvons alors construire les vecteurs de contexte $p^c$ et $h^c$ suivants pour la prémisse et l'hypothèse :
		\begin{equation}
		\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{N} exp(e_{ik})} , p_i^c = \sum_{j=1}^{N} \alpha_{ij} h_j^s   
		\end{equation}
		\begin{equation}
		\beta_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{M} exp(e_{kj})} , h_j^c = \sum_{i=1}^{M} \beta_{ij} p_i^s   
		\end{equation}
		où $M$ est la longueur de la prémisse, $N$ est la longueur de l'hypothèse, $\alpha \in \mathbb{R}^{M \times N}$ est un $softmax(e)$ 
		sur la prémisse, et $\beta \in \mathbb{R}^{M \times N}$ est un $softmax(e)$ sur l'hypothèse. Ceci permet à la prémisse de voir le contexte de l'hypothèse 
		et vice-versa.\\
		Avec ces nouvelles représentations pour les mots, on effectue du mean-pooling :
		\begin{equation}
		pool_p = \frac{\sum_{i=1}^{N} p_i^c}{N} , pool_h = \frac{\sum_{i=1}^{M} h_i^c}{M}
		\end{equation}
		puis on effectue une concaténation du mean-pooling de la prémisse et de l'hypothèse pour l'envoyer à une couche de décision :
		\begin{equation}
		y = softmax( W \times [ pool_p ; pool_h ] + b )
		\end{equation}		
		
		\\
		\\
		Les performances de ces systèmes et autres détails techniques 
		sont décrits dans la partie "résultats expérimentaux" en annexe.\\
		Les schémas de ces systèmes figurent dans la partie "" en annexe.

\section{Définition d'interprétabilité}
	Il n'y a malheureusement pas de consensus concernant la définition d' "interprétabilité". 
	Miller définit cela comme étant le degré auquel un humain peut comprendre la cause d'une décision\cite{ref4}. 
	Un système a donc une meilleure interprétabilité qu'un autre si ses explications sont plus faciles à comprendre par un humain.
	Par ailleurs, on peut interpréter de manière globale en expliquant le comportement général du class
	\subsection{Qu'est-ce-qu'une explication ?}
		La définition donnée par Miller est assez simple : une explication est une réponse à une question commençant par "pourquoi". 
		Une question commençant par "comment" peut être retournée en une question commençant par "pourquoi".
		Le terme "explication" désigne le processus social et cognitif d'expliquer, mais c'est également le produit de ces processus.
	\subsection{Qu'est-ce-qu'une "bonne" explication ?}
		La définition d'une bonne explication ne doit pas se baser sur l'intuition de l'auteur, mais plutôt sur des faits. 
		Miller résume ce qu'est une bonne explication\cite{ref5}, basée sur ce que les humains attendent d'une explication. 
		Grâce à cela, nous allons énoncer les types d'explications adéquats à notre projet. Cependant, 
		il ne faut pas oublier que les humains ont tendance à rejeter toutes explications allant à l'encontre de leur croyance.
	\subsubsection{Explication contrastée}
		C'est une explication qui doit être comparée. Les utilisateurs se demandent généralement pourquoi cette prédiction a 
		été faite et pas une autre, via la question "quelle aurait été la prédiction si cette entrée avait été changé 
		par une autre ?".\\
		Un docteur se demandant "pourquoi ce traitement ne marche pas sur ce patient ?" voudrait comparer les données de ce patient 
		à un autre patient ayant des caractéristiques similaires mais pour qui le traitement marche.\\
		La meilleure explication pour ce type d'explication est celle qui met en évidence les différences entre l'entrée traitée et 
		l'entrée de comparaison.\\
		L'entrée de comparaison peut être artificielle.
	\subsubsection{Explication sélective}
		C'est une explication qui doit être courte. Généralement, on peut expliquer un phénomène par plusieurs facteurs. 
		Il faut en donner peu, à savoir deux ou trois raisons, même si les explications
		peuvent être plus complexes que cela.
	\subsubsection{Explication sociale}
		Comme nous l'avons expliqué ci-dessus, une explication est un processus social, 
		c'est-à-dire qu'il faut prendre en compte les connaissances de la personne à qui l'on veut donner une explication. 
		Dans notre projet, nous partons du principe qu'une explication doit être comprise par tout le monde, 
		que ce soit par un expert du domaine de l'apprentissage automatique ou bien par quelqu'un qui n'en a jamais entendu parler.
	\begin{comment}
	\subsubsection{Explication anormale}
		Les explications "anormales" sont beaucoup appréciées, c'est-à-dire que si une cause rare a influencé la prédiction, 
		il faut la spécifier. Dans notre projet, une cause rare peut être un mot -ou un groupe de mot- de l'hypothèse qui ne peut pas 
		être mis en relation avec un mot -ou un groupe de mot- de la prémisse, mais qui a influencé la prédiction.
	\end{comment}

\section{Techniques pour l'interprétabilité}	
	Dans cette section nous allons décrire notre technique pour interpreter une prédiction d'une entrée. Nous voulons donner des explications pour chaque label 
	en donnant les trois mots les plus importants dans la prémisse et les trois mots les plus importants dans l'hypothèse. On veut donc calculer l'importance de chaque mot. 
	L'importance d'un mot correspond à l'importance de sa contribution pour le label $y$.\\
	Notre intuition est la suivante : 
	On veut donner une explication pour le label $y$ et l'entrée $x$ composée de la prémisse et de l'hypothèse. 
	On retire alors un mot et on demande à notre modèle de nous donner les probabilités de chaque label avec cette nouvelle entrée. Si la probabilité 
	du label $y$ a baissé, alors le mot était important : cela veut dire que le mot avait contribué au label $y$. A l'inverse, si elle a augmenté, le mot n'avait donc pas contribué au label $y$. 
	De plus, si la probabilité des autres labels a augmenté, alors le mot a d'autant plus d'importance : en le retirant, l'entrée $x$ a basculé vers un autre label.\\
	\\
	L'importance d'un mot, que nous notons IF pour Impact Factor, est calculé comme suit :
	\begin{equation}
	IF^{y_j}(m_i) = \sum_{j=1}^{Y_n} \log( p(y_j | x \backslash m_i) ) - \log( p(y_j | x) ) 
	\end{equation}
	\begin{equation}
	IF^{y_{ref}}(m_i) = -\log( p(y_{ref} | x \backslash m_i) ) + \log( p(y_{ref} | x) )
	\end{equation}
	\begin{equation}
	IF^{total}(m_i) = IF^{y_{ref}}(m_i) + IF^{y_j}(m_i) 
	\end{equation}\\
	où $y_{ref}$ est le label de référence (le label que l'on veut "expliquer"), $x$ est l'entrée, $m_i$ est le mot que l'on retire de l'entrée $x$, 
	$y_j$ est un label différent de $y_{ref}$, et $Y_n$ est le nombre de label. Les probabilités sont exprimées en log-probabilités pour des soucis de précision et d'optimisation du code. On parle donc plutôt de score.\\
	Cette formule suit notre intuition de base et également celle de Robnik-Sikonja et Kononenko \cite{ref8}. Nous avons rajouté la prise en compte de l'impact sur les scores 
	des autres labels quand on retire le mot $m_i$.\\
	\\
	Un problème persiste tout de même : que se passe-t-il vraiment lorsque l'on retire un mot ? Nous ne pouvons pas simplement le supprimer de l'entrée, car la phrase n'aurait plus de sens 
	et le modèle n'est peut-être pas entraîné à faire face aux bruits. On ne peut pas vraiment le remplacer par un mot générique (le mot "UNK" pour "mot inconnu", par exemple) 
	car un modèle remplacant ce mot par un embedding à 0 et un modèle remplacant ce mot par un embedding particulier auront des réponses très différentes. Or on voudrait que notre méthode soit 
	réalisable par n'importe quel modèle. La solution est donc de remplacer le mot que l'on veut retirer par un autre :
	\begin{equation}
	p(y | x \backslash m_i) =  \sum_{s=1}^{r} p(y | x \leftarrow m_i = m_s) 
	\end{equation}\\
	où $r$ est le nombre de mot que l'on peut mettre à la place du mot $m_i$ et $m_s$ est le mot que l'on met à la place de $m_i$.\\
	Les mots pouvant remplacer $m_i$ sont situés dans un fichier externe créé par nous-même. On a fixé à 5 le nombre de mots maximum pouvant remplacer $m_i$.
	Plus il y a de mots de remplacements, plus on a une meilleure estimation du IF du mot $m_i$, mais dans ce cas la complexité en temps augmente. Il faut donc trouver un compromis entre 
	l'estimation du IF et le temps que met le programme pour donner une explication.\\
	\\
	Cette méthode permet donc d'avoir une explication contrastée, puisque l'on compare notre entrée de base avec des entrées créées artificiellement en remplacant un mot par un autre. 
	De plus, elle est également sélective puisque l'on sélectionne les trois mots, respectivement dans la prémisse puis dans l'hypothèse, ayant le plus contribué au label $y_{ref}$. 
	Enfin, le programme surligne les mots les plus importants pour chaque label, ce qui permet d'avoir une visualisation et donc d'être compris par n'importe qui.
	
	/* METTRE UN SCREEN D'UNE EXPLICATION */
 
\bibliographystyle{unsrt} % Le style est mis entre accolades.
\bibliography{biblio}

\renewcommand{\appendixname}{Annexes}
\appendix
\section{Résulats expérimentaux}
Pour la représentation des mots, nous utilisons des words embeddings pré-entrainés de dimension 100 via GloVe.6B.100d.
Pour les mots inconnus, c'est-à-dire les mots qui n'ont pas d'embedding dans GloVe, nous utilisons des embeddings
initialisés au hasard. Tous les embeddings sont mis-à-jour par le réseau.\\
La taille des batches est de 16 et le dropout est à 0,3.\\
Concernant les RNNs utilisés, il n'y a qu'une seule couche et la dimension des états cachés est de 100.\\

\begin{tabular}{|c|p{3cm}|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
\hline
\small{Système} & \small{Taux de réussite Contradiction} & \small{Taux de réussite Inférence} & \small{Taux de réussite Neutre} & \small{Taux de réussite Dev} & \small{Taux de réussite Test} \\
\hline
1 & 67,33\% & 73,90\% & 68,38\% & 69,89\% & test \\
\hline
2 & 78,55\% & 87,98\% & 74,99\% & 80,57\% & test \\
\hline
3 & 68,73\% & 74,92\% & 69,55\% & 71,09\% & test \\
\hline
\end{tabular}

%pour taux réussite test sys2
%0.800712
	%Contradiction Accuracy = 0.777023
	%Inference Accuracy = 0.861045
	%Neutral Accuracy = 0.761417

\end{document}














