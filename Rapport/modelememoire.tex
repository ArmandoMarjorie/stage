

% Les packages utilise's ci-dessous le sont a` titre indicatif ;
% vous pouvez les changer a` votre convenance.

% Le type de document: article, rapport...
\documentclass[a4paper]{article}

% Mettre les diffe'rents packages et fonctions que l'on utilise
%\usepackage[english]{babel}
\usepackage[french]{babel}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{color}
\usepackage{graphicx}
\usepackage{tabularx}
\usepackage[nottoc, notlof, notlot]{tocbibind}
\usepackage{appendix}
\usepackage{comment}


% Commenter l'une de ces deux lignes
%\RequirePackage[applemac]{inputenc}
\RequirePackage[latin1]{inputenc}
\usepackage[T1]{fontenc}

\begin{document}


%----------- A   C O M P L E T E R   P A R   L E S   A U T E U R S ------------


% Titre du rapport
\def\TitreRapport{
    Interprétation de prédictions de modèles pour l'inférence textuelle 
    en perturbant significativement les entrées
}

% Pre'nom et nom de l'autrice
\def\NomsAuteurs{
    Marjorie Armando
}

% Date du rapport (dans la me^me langue que le titre)
\def\DateRapport{
    1 juin 2018
}

% Nom des encadrants
\def\Encadrants{
    \textbf{Encadrant} \\
    Benoit Favre
}
% Nom du laboratoire
\def\Labo{
    Laboratoire d'Informatique et des Systèmes - LIS
}



% Re'sume' en franc,ais avec mots-cle's
\def\ResumeFrancais{
    La reconnaissance de l'inférence textuelle (Recognizing Textual Entailment : RTE) est au coeur de tous les aspects de 
    la compréhension de texte en traitement automatique du langage (TAL). 
    Le but de la RTE est de savoir automatiquement si une phrase, appelée l'hypothèse, est 
    déduite d'une autre phrase, appelée la prémisse. Pour résoudre cela, on utilise des systèmes issus de l'apprentissage 
    automatique.\\
    Le problème qui se pose avec l'utilisation de ces systèmes est que le modèle nous 
    fournit uniquement les probabilités de chaque label. Ainsi, aucune information supplémentaire n'est
    donnée : comment savoir, de manière humainement compréhensible, pourquoi le modèle a associé un label 
    particulier à une paire de phrases donnée ?\\Pouvoir répondre à cette question permettrait de rendre un modèle utilisable 
    dans des domaines où les décisions doivent être mûrement réfléchies telle que la médecine, car malgré l'expansion des
    réseaux de neurones, ceux-ci restent des boîtes noires et il est donc difficile de leur faire confiance sans avoir
    d'explications en retour.\\
    Dans ce travail, nous proposons une méthode respectant les règles d'une "bonne" explication 
    pour rendre un modèle interprétable dans le cadre de la RTE avec l'utilisation du corpus SNLI \cite{ref}.
    Nous allons la comparer avec la méthode Local Interpretable Model-agnostic Explanations (LIME) \cite{ref2} qui permet
    d'expliquer les prédictions de n'importe quel classifieur en apprenant localement un modèle interprétable dans le voisinage
    de l'entrée.  
    \\[2mm]
    {\bf Mots-clés : } inférence textuelle, apprentissage automatique,\\traitement automatique des langues naturelles, 
    interprétabilité, LSTM
}


\thispagestyle{empty}
\begin{center}
\baselineskip=1.3\normalbaselineskip
{\bf\Large \TitreRapport}\\[8mm]
{\bf\large \NomsAuteurs}\\[1mm]
{\Labo}\\[4mm]
\Encadrants\\[10mm]

{\bf Résumé}
\end{center}

\ResumeFrancais\\[4mm]

\newpage

%-------------------- T E X T E   D U   R A P P O R T -------------------------

\section{Introduction}
	\subsection{Contexte de l'étude}
		Les concepts sémantiques d'inférence et de contradiction sont au coeur de tous les aspects de la compréhension de texte en TAL. 
		Ainsi, l'inférence textuelle en langage naturel (Natural Language Inference : NLI), caractérisant et utilisant ces relations dans 
		les systèmes computationnels, est essentielle dans des tâches telles que la recherche d'information, le raisonnement de bon sens, ou encore le système 
		de question/réponse.\\
		\\
		Plus spécifiquement, en ayant une paire de phrase prémisse/hypothèse, la RTE se voit comme 
		objectif de détecter si la seconde phrase est en contradiction, se déduit ou bien est neutre par rapport à la 
		première phrase. Nous avons donc trois étiquettes permettant d'illustrer la 
		relation entre la prémisse et l'hypothèse : \\
		\begin{itemize}\renewcommand{\labelitemi}{$\bullet$}
			\item Contradiction : l'hypothèse contredit la prémisse. Par exemple :\\
				\textit{
				Prémisse : "Le chat est entièrement blanc."\\ 
				Hypothèse : "Le chat est entièrement noir."}\\

			\item Neutre : l'hypothèse est possible dans le contexte de la prémisse. Par exemple :\\
				\textit{
				Prémisse : "Le chat dort sur la banquette."\\
				Hypothèse : "Le chat aime le chocolat."}\\

			\item Inférence : l'hypothèse est déduite de la prémisse. Par exemple :\\
				\textit{
				Prémisse : "Le chat aimerait manger la sourie."\\
				Hypothèse : "Le chat a faim."}\\
		\end{itemize} \\
		\\
		\\
		Pour résoudre le problème de la RTE, on utilise des systèmes issus de l'apprentissage automatique. Nous utilisons les réseaux de neurones 
		récurrents (Recurrent Neural Network : RNN) dans ce projet.\\

	\subsection{Problématique}
		Le problème qui se pose avec l'utilisation de système issue de l'apprentissage automatique est que le modèle nous fournit 
		uniquement les probabilités de chaque label. Comment savoir, de manière humainement compréhensible, pourquoi le modèle a 
		associé un label particulier à une paire de phrases donnée ?\\
		\\
		Pouvoir répondre à cette question permettrait de rendre un modèle utilisable dans des domaines où les décisions doivent être 
		mûrement réfléchies, telle que la médecine. Si un modèle propose de donner un certain traitement à un patient, 
		il faut que le modèle puisse donner de bonnes explications pour que le docteur l'approuve, car les conséquences pourraient 
		être catastrophique. \\
		\\
		Ainsi, un modèle doit être digne de confiance. Cette confiance donnée par les humains à un système dépend des explications 
		données. Nous verrons dans la partie "Interprétabilité" quels sont les critères d'une "bonne" explication. De plus, si un système est 
		jugé digne de confiance, il sera davantage utilisé : en effet, il a été observé, par exemple, que le fait de fournir des 
		explications augmente l'acceptation des recommandations de films \cite{ref2}. \\
		\\
		Pouvoir donner une interprétation du système peut donc permettre aux utilisateurs d'adopter un modèle. 
		Mais cela peut également aider le développeur lors du choix d'un modèle parmi ceux qu'il a implémenté, 
		car le taux de réussite n'est pas le seul critère à prendre en compte : un modèle peut fournir le label attendu 
		en se basant sur de mauvaises informations.\\
		\\
		Dans ce rapport, nous allons tout d'abord énumérer quelques méthodes existantes pour l'interprétation de prédictions d'un modèle.
		Nous allons ensuite spécifier les modèles que nous avons implémenté pour la RTE ainsi que les corpus utilisé, puis nous allons définir ce que nous devons 
		attendre d'une "bonne" explication. Puis, nous allons expliciter notre technique pour l'interprétation d'une prédiction d'un modèle, et 
		nous allons la comparer avec LIME. Enfin, nous allons conclure avec les différentes perspectives. 
		
		%\bigskip

\section{Etat de l'art}
	\subsection{Dans le domaine de la RTE}
		Les premiers travaux sur la RTE ont été 
		formé sur de très petits ensembles de données avec des méthodes conventionnelles, 
		telles que les méthodes peu profondes \cite{ref9} ou encore
		les méthodes de logique naturelle \cite{ref10}.
		\\
		Ces dernières années, on dénote une nette amélioration pour la tâche de la RTE, en particulier 
		grâce à la publication du corpus SNLI (the Stanford Natural Language Inference) qui contient 
		570K paires de phrases annotées. Cela a amélioré la faisabilité d'entrainer des réseaux de neurones 
		complexes, puisqu'ils nécessitent souvent une quantité relativement importante de données. En 
		effet, la recherche sur l'apprentissage automatique dans ce domaine été limitée 
		par le manque de corpus assez grand.\\
		\\
		On peut distinguer deux types d'approches parmis ces modèles : les modèles basés sur l'encodage 
		des phrases 
		et les modèles axés sur l'attention inter-phrases.\\
		Les premiers types de modèles encodent les phrases puis un classifieur (un  
		perceptron multi-couches) décide de la relation entre ces deux phrases encodées. Différents encodeurs ont été 
		proposés, tels que les Long-Short Term Memory (LSTM) \cite{ref}, les Gated Recurrent Unit 
		(GRU) \cite{ref11}, les réseaux de neurones à convolution (Convolutionnal Neural 
		Network : CNN) \cite{ref12}, les Bidirectionnal Long-Short Term Memory (BiLSTM) et ses 
		variantes \cite{ref13} \cite{ref14} \cite{ref15}, et des réseaux neuronaux plus 
		complexes \cite{ref16} \cite{ref17}.\\
		L'avantage de ces modèles est que les encodeurs transforment les phrases 
		en vecteur de taille fixe, ce qui peut aider à un large éventail de tâches 
		de transfert \cite{ref18}. Cependant, cette architecture ignore l'interaction 
		locale entre deux phrases, pourtant nécessaire \cite{ref10}.\\
		Les seconds modèles ont alors été proposé pour éviter ce problème.  
		Dans ce cadre, l'information d'inférence locale est collectée par le mécanisme d'attention 
		puis introduite dans des réseaux de neurones pour composer des vecteurs de taille fixe avant la 
		classification finale. Beaucoup suivent cette route. Parmis eux, Rocktäschel et al. \cite{ref19} 
		ont été les premiers à proposer des réseaux basés sur l'attention pour la RTE. 
		Chen et al. \cite{ref20} ont proposé un modèle d'inférence séquentielle amélioré qui est 
		l'un des meilleurs modèles à ce jour (88,6\% de taux de réussite).
		
	\subsection{Dans l'interprétabilité pour le TAL}
		Dans la pratique, il y a souvent un compromis entre le taux de réussite et l'interprétabilité du modèle.
		Certains modèles tels que les arbres de décisions ou encore les modèle linéaires sont facilement interprétables, 
		et sont donc parfois utilisés à la place de modèles complexes tels que les réseaux de neurones profonds,  
		même si ceux-ci peuvent donner de meilleurs résultats. Cependant, au cours de 
		ces dernières années, de grandes avancées 
		ont été effectuées pour tenter d'interpréter les modèles utilisés jusqu'alors comme des boîtes noires.\\
		\\
		Nous étudions dans ce rapport la méthode LIME qui permet d'expliquer les prédictions de n'importe quel classifieur 
		ou regresseur. L'objectif global de LIME est d'identifier un modèle interprétable parmi le voisinage de l'entrée $x$.\\
		\\ 
		Tout d'abord, on distingue les features utilisées aux représentations interprétables des features. Par exemple, 
		les features sont les embeddings des mots et la représentation interprétable de ces features est un vecteur binaire 
		qui indique la présence ou l'absence des mots.\\
		\\
		LIME définit une explication par un modèle $g \in G$, où $G$ est la classe des modèles interprétables tels que 
		les modèles linéaires ou les arbres de décisions. Vu que les modèles interprétables n'ont pas tous la même difficulté 
		à être interprété, LIME définit $\Omega(g)$ qui est une mesure de la complexité d'interpréter $g$. En 
		prennant l'example des arbres de décisions, $\Omega(g)$ est la profondeur.\\
		On dénote par $f : \mathbb{R}^{d} \rightarrow \mathbb{R}$ le modèle utilisé comme une boîte noire. $f(x)$ est la 
		probabilité que l'entrée $x$ appartienne à une certaine classe.\\
		LIME va alors se baser sur la représentation interprétable des données en retirant un mot au hasard. On dénote cette 
		nouvelle entrée par $z$. On définit la localité de $x$ avec $\pi_x(z)$ qui est une mesure de proximité entre $z$ et 
		$x$. C'est un noyau se basant sur la similarité cosinus.\\
		Enfin, on définit $L(f, g, \pi_x)$ qui est une mesure pour savoir à combien $g$ est infidèle à $f$ dans la localité 
		défini par $\pi_x$. Pour préserver à la fois l'interprétabilité et la fidèlité locale, LIME minimise $L(f, g, \pi_x)$ 
		avec $\Omega(g)$ assez petit pour être interprétable par les humains. L'explication de LIME est donc la suivante :
		\begin{equation}
		\varepsilon(x) = \underset{g \in G}{argmin} L(f, g, \pi_x) + \Omega(g)
		\end{equation}		
		Cette formule peut être utilisée par différents modèles $g \in G$, fonctions de 
		fidèlité $L(f, g, \pi_x)$, et mesure de complexité $\Omega(g)$.\\
		\\
		LIME peut alors donner les $K$ mots les plus importants de l'entrée $x$ pour tout label.\\
		La figure ci-dessous est un exemple illustant le principe de LIME :
		
		\begin{figure}[h]
			\center
			\includegraphics[width=8cm, height=6cm, keepaspectratio]{lime_exemple.png}
			\caption{Exemple présentant l'intuition de LIME.}
		\end{figure}
		\\
		La décision de la boîte noire $f$, inconnu par LIME, est représentée par le fond bleu et rose. 
		La croix rouge en gras est l'entrée $x$ que l'on veut expliquer. LIME crée des entrées modifiées, 
		utilise $f$ pour avoir la probabilité de ces entrées pour le label $y$, et les pondère par leur proximité 
		par rapport à $x$ (les poids sont représentés par la taille). La droite pointillée est l'explication apprise qui est 
		localement fidèle.\\
		
	\subsection{Dans l'interprétabilité de la RTE}
		Ce n'est que très recemment que Silva et al. \cite{ref21} 
		
		/*PARLER ARTICLE LREC*/
		
		\begin{comment}
		D'autres méthodes d'interprétation existent, telle que DeepLIFT (Deep Learning Important FeaTures :
		DeepLIFT) \cite{ref9} pour l'apprentissage profond, 
		qui décompose la prédiction d'un 
		réseau de neurones sur une entrée spécifique en rétropropageant les contributions de tous les neurones du réseau à 
		chaque feature de l'entrée. DeepLIFT compare l'activation de chaque neurone à son "activation de référence" et 
		attribue des scores de contribution en fonction de la différence. L'activation de référence est choisi par 
		l'utilisateur.\\
		On peut également citer la méthode de propagation de pertinence par couche \cite{ref10} pour l'interprétation des 
		réseaux de neurones profonds, une méthode équivalente à DeepLIFT avec l'activation de référence de tous les neurones 
		fixée à 0. 
		\end{comment}
		
	
		
		\begin{comment}
		mais nous pouvons également citer la méthode SHapley Additive exPlanations (SHAP) \cite{ref8} : 
		elle explique la prédiction de n'importe quel modèle en utilisant les valeurs de Shapley, introduites dans la théorie des jeux en 1953. 
		Ces valeurs ont récemment été utilisées pour attribuer une mesure d'importance aux features \cite{ref5}. SHAP appartient à la classe des "méthodes d'attribution de features additives" : 
		elle attribue une valeur à chaque feature pour chaque prédiction (c'est-à-dire une attribution de feature). 
		Plus la valeur est haute, plus l'attribution de la feature à la prédiction spécifique est grande :  
		la somme de ces valeurs devrait donc être proche de la prédiction initiale du modèle.\\
		\\
		SHAP a rassemblé et unifié six méthodes d'attribution de features additives, dont LIME, et est la seule méthode qui respecte les trois propriétés suivantes :
		\begin{description}
			\item[- Précision locale] : les explications sont fidèles et véridiques au modèle.\\
			\item[- Feature manquante] : les features retirées n'ont aucun impact attribué aux prédictions du modèle.\\
			\item[- Cohérence] : les explications sont cohérentes avec l'intuition humaine. Techniquement, la cohérence indique que si un modèle change de sorte que la 
			contribution d'une entrée augmente ou reste la même indépendamment des autres entrées, l'attribution de cette entrée ne devrait pas diminuer.\\
		\end{description}\\
		\\
		Le calcul de ces valeurs est cependant très coûteux : il faut entraîner le modèle sur tous les sous-ensembles de features S inclu F, où F est l'ensemble de toutes les features. 
		Les valeurs Shapley attribuent une valeur d'importance à chaque feature, ce qui représente l'effet sur la prédiction du modèle d'inclure cette feature. 
		Pour cela, un modèle est entraîné avec la feature présente, et un autre modèle est entraîné avec la feature retirée. 
		Ensuite, les prédictions des deux modèles sont comparées sur l'entrée courante, c'est-à-dire qu'on calcule leur différence. 
		Comme l'effet de la supression d'une feature dépend d'autres features du modèle, les différences précédentes sont 
		calculées pour tous les sous-ensembles possibles de features. Les valeurs de Shapley sont une moyenne pondérée de toutes 
		les différences possibles et sont utilisées comme attribution de feature.\\
		\\
		Un algorithme efficace a été mis au point, cependant il fonctionne uniquement sur les modèles basés sur les arbres.
		\end{comment}

\section{Définition d'interprétabilité}
	Il n'y a malheureusement pas de consensus concernant la définition d' "interprétabilité". 
	Miller définit cela comme étant le degré auquel un humain peut comprendre la cause d'une décision \cite{ref4}. 
	Un système a donc une meilleure interprétabilité qu'un autre si ses explications sont plus faciles à comprendre par un humain.
	Par ailleurs, on peut interpréter de manière globale en expliquant le comportement général du modèle, ou de manière locale en expliquant 
	pourquoi le modèle a choisi tel label. Dans ce travail, nous nous placons dans le cas de l'interprétation locale : le but est d'expliquer 
	pourquoi le modèle a choisi un label $y$ pour une entrée $x$.
	
	\begin{figure}[h]
		\center
		\includegraphics[width=10cm, height=6cm, keepaspectratio]{xai-figure.png}
		\caption{Concept des IAs explicables (eXplainable AI : XAI) \cite{ref22}.}
	\end{figure}	
	
	\subsection{Qu'est-ce-qu'une explication ?}
		La définition donnée par Miller est assez simple : une explication est une réponse à une question commençant par "pourquoi". 
		Une question commençant par "comment" peut être retournée en une question commençant par "pourquoi".
		Le terme "explication" désigne le processus social et cognitif d'expliquer, mais c'est également le produit de ces processus.
	\subsection{Qu'est-ce-qu'une "bonne" explication ?}
		La définition d'une bonne explication ne doit pas se baser sur l'intuition de l'auteur, mais plutôt sur des faits. 
		Miller résume ce qu'est une bonne explication \cite{ref5}, basée sur ce que les humains attendent d'une explication. 
		Grâce à cela, nous allons énoncer les types d'explications adéquats à notre projet. Cependant, 
		il ne faut pas oublier que les humains ont tendance à rejeter toutes explications allant à l'encontre de leur croyance.
	\subsubsection{Explication contrastée}
		C'est une explication qui doit être comparée. Les utilisateurs se demandent généralement pourquoi cette prédiction a 
		été faite et pas une autre, via la question "quelle aurait été la prédiction si cette entrée avait été changé 
		par une autre ?".\\
		Un docteur se demandant "pourquoi ce traitement ne marche pas sur ce patient ?" voudrait comparer les données de ce patient 
		à un autre patient ayant des caractéristiques similaires mais pour qui le traitement marche.\\
		La meilleure explication pour ce type d'explication est celle qui met en évidence les différences entre l'entrée traitée et 
		l'entrée de comparaison.\\
		L'entrée de comparaison peut être artificielle.
	\subsubsection{Explication sélective}
		C'est une explication qui doit être courte. Généralement, on peut expliquer un phénomène par plusieurs facteurs. 
		Il faut en donner peu, à savoir deux ou trois raisons, même si les explications
		peuvent être plus complexes que cela.
	\subsubsection{Explication sociale}
		Comme nous l'avons expliqué ci-dessus, une explication est un processus social, 
		c'est-à-dire qu'il faut prendre en compte les connaissances de la personne à qui l'on veut donner une explication. 
		Dans notre projet, nous partons du principe qu'une explication doit être comprise par tout le monde, 
		que ce soit par un expert du domaine de l'apprentissage automatique ou bien par quelqu'un qui n'en a jamais entendu parler.
	\begin{comment}
	\subsubsection{Explication anormale}
		Les explications "anormales" sont beaucoup appréciées, c'est-à-dire que si une cause rare a influencé la prédiction, 
		il faut la spécifier. Dans notre projet, une cause rare peut être un mot -ou un groupe de mot- de l'hypothèse qui ne peut pas 
		être mis en relation avec un mot -ou un groupe de mot- de la prémisse, mais qui a influencé la prédiction.
	\end{comment}
	
\section{Réseaux de neurones récurrents}
	\subsection{Données externes utilisées}
		Nous utilisons les corpus SNLI composés d'un fichier d'entraînement, de validation et de test. 
		Chaque fichier est en format json et en texte brut. Nous avons donc pu facilement les tokeniser.\\
		Ces corpus ont été réalisé par 5 annotateurs à l'aide d'une image accompagnée d'un texte bref -la prémisse- présentant 
		la dite image. Les annotateurs ont alors écrit une phrase étant neutre par rapport à la prémisse, une autre 
		étant en contradiction et une autre phrase qui pouvait être déduite de la prémisse : ils ont donné une hypothèse 
		et un label. Pour que le corpus ne soit pas trop subjectif, les annotateurs ont eu accès à quelques paires 
		prémisse/hypothèse sans label. Chacun d'entre eux a donné un label, celui ayant eu le plus de voix a été décidé comme 
		le label gold de la paire observée. Ainsi, certaines paires n'ont pas de label car les annotateurs n'ont pas trouvé de 
		consensus : nous ne prennons pas en compte ce genre d'entrée.\\
		La table ci-dessous est un échantillon du corpus SNLI présentant à gauche la prémisse, 
		à droite l'hypothèse, et au centre les labels des 5 annotateurs avec le label gold :\\
		\begin{figure}[h]
			\center
			\includegraphics[width=13cm, height=10cm, keepaspectratio]{table_SNLI.png}
			\caption{Echantillon du corpus de développement de SNLI.}
		\end{figure}\\
		\\
		Concernant les embeddings des mots, nous utilisons les embeddings pré-entraînés de GloVe. Les mots ne se trouvant 
		pas dans le vocabulaire de GloVe ont des embeddings initialisés au hasard.
	\begin{comment}
	\subsection{LSTM et Bi-LSTM}
		Dans ce travail, nous utilisons les réseaux de neurones récurrents, 
		et plus particulièrement les LSTMs (Long Short-Term Memory : LSTM) et les Bi-LSTMs (Bidirectionnal 
		Long Short-Term Memory : Bi-LSTM).\\ 
		Ils permettent de relier des informations vues antérieurement à la tâche 
		courante. On leur donne des informations à chaque instant $t$, créant ainsi une 
		cellule.\\
		\begin{figure}[h]
			\center
			\includegraphics{lstm_schema_.png}
			\caption{Schéma d'un LSTM contenant trois cellules.}
		\end{figure}\\
		Ce sont des RNNs particuliers capable d'apprendre une dépendance 
		très éloignée à notre tâche courante : ils ont la capacité de supprimer ou 
		d'ajouter des informations à l'état de la cellule, régulé par des portes 
		qui décident s'il faut laisser passer de l'information.\\
		Ces réseaux de neurones sont utilisés ici en tant qu'encodeur : on fournit en entrée, à chaque instant $t$, 
		l'embedding du t\up{ième} mot de la phrase traitée, à savoir la prémisse ou l'hypothèse. 
		Le réseau calcule alors à chaque instant $t$ un état caché $h_t$ comme suit :\\
		\begin{equation}
		i_t = \sigma(W_i x_t + U_i h_{t-1} + b_i)
		\end{equation}
		\begin{equation}
		f_t = \sigma(W_f x_t + U_f h_{t-1} + b_f)
		\end{equation}
		\begin{equation}
		u_t = \tanh(W_u x_t + U_u h_{t-1} + b_u)
		\end{equation}
		\begin{equation}
		o_t = \sigma(W_o x_t + U_o h_{t-1} + b_o)
		\end{equation}
		\begin{equation}
		c_t = f_t \odot c_{t-1} + i_t \odot u_t
		\end{equation}
		\begin{equation}
		h_t = o_t \odot \tanh(c_t)
		\end{equation}
		où $\sigma$ est la fonction sigmoid, $\odot$ est le produit de Hadamard entre deux vecteurs, 
		$W_i$, $W_f$, $W_u$, $W_o \in \mathbb{R}^{D \times D_e}$, 
		$U_i$, $U_f$, $U_u$, $U_o \in \mathbb{R}^{D \times D}$ et 
		$b_i$, $b_f$, $b_u$, $b_o \in \mathbb{R}^{D}$ sont des paramètres mis-à-jour par le réseau. $D$ est la dimension 
		des états cachés et $D_e$ est la dimension des embeddings.

	\subsection{Systèmes implémentés}
	\end{comment}
	\subsection{Systèmes implémentés}
		Nous avons implémenté 3 systèmes différents avec la librairie DyNet \cite{ref6} en C++. 
		Nous utilisons les LSTMs et les BiLSTMs :\\
		\\
		Le premier système passe la prémisse et l'hypothèse au LSTM pour avoir une représentation pour chacune de ces deux phrases.  
		On les concatène pour les envoyer ensuite à une couche de décision : 
		\begin{equation}
		y = softmax( W \times [ LSTM(\textit{prémisse}) ; LSTM(\textit{hypothèse}) ] + b )
		\end{equation}
		où $y$ est un vecteur contenant les probabilités de chaque label, $W$ est la matrice de poids, $LSTM(\textit{prémisse})$ et $LSTM(\textit{hypothèse})$ sont respectivement la représentation de 
		la prémisse et de l'hypothèse, $b$ est le biais, et [;] dénote la concaténation. \\
		\\
		\\
		Le deuxième système effectue le même mécanisme que le premier système pour avoir une représentation de la prémisse et de 
		l'hypothèse. On compare les deux représentations pour envoyer la comparaison à une couche de décision :
		\begin{equation}
		y = softmax( W \times ( LSTM(\textit{prémisse}) \times LSTM(\textit{hypothèse})^{T} ) + b )
		\end{equation}
		où $T$ dénote la transposée.\\
		\\
		\\
		Le troisième système est inspiré de la méthode KIM \cite{ref3}.\\ 
		On représente les mots de la prémisse et de l'hypothèse en les passant dans un Bi-LSTM : il utilise un LSTM forward pour 
		lire la phrase de gauche à droite, puis un LSTM backward pour lire la phrase dans l'autre sens. A chaque mot lu, un 
		état caché est généré par les deux LSTMs. Ces deux états cachés sont alors concaténés pour obtenir une 
		représentation du mot :\\
		$h_t = [\overset{\rightarrow}{h_t} ; \overset{\leftarrow}{h_t}]$, 
		où $\overset{\rightarrow}{h_t}$ est l'état caché généré par le LSTM forward à 
		l'instant t, $\overset{\leftarrow}{h_t}$ est celui généré par le LSTM backward à l'instant $t$, et $h_t$ est la 
		représentation du mot $t$.\\
		On dénote par $p^s$ (respectivement $h^s$) le vecteur de représentation des mots de la prémisse 
		(respectivement de l'hypothèse).\\
		Nous construisons ensuite une matrice d'alignement comme suit :
		\begin{equation}
		e_{ij} = (p_i^s)^T h_j^s
		\end{equation}
		où $p_i^s$ est la représentation du i\up{ème} mot de la prémisse, $h_j^s$ est celle du j\up{ième} mot de l'hypothèse.\\
		Avec cette matrice, nous pouvons alors construire les vecteurs de contexte $p^c$ et $h^c$ suivants pour la prémisse et l'hypothèse :
		\begin{equation}
		\alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{N} exp(e_{ik})} , p_i^c = \sum_{j=1}^{N} \alpha_{ij} h_j^s   
		\end{equation}
		\begin{equation}
		\beta_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{M} exp(e_{kj})} , h_j^c = \sum_{i=1}^{M} \beta_{ij} p_i^s   
		\end{equation}
		où $M$ est la longueur de la prémisse, $N$ est la longueur de l'hypothèse, $\alpha \in \mathbb{R}^{M \times N}$ est un $softmax(e)$ 
		sur la prémisse, et $\beta \in \mathbb{R}^{M \times N}$ est un $softmax(e)$ sur l'hypothèse. Ceci permet à la prémisse de voir le contexte de l'hypothèse 
		et vice-versa.\\
		Avec ces nouvelles représentations pour les mots, on effectue du mean-pooling :
		\begin{equation}
		pool_p = \frac{\sum_{i=1}^{N} p_i^c}{N} , pool_h = \frac{\sum_{i=1}^{M} h_i^c}{M}
		\end{equation}
		puis on effectue une concaténation du mean-pooling de la prémisse et de l'hypothèse pour l'envoyer à une couche de décision :
		\begin{equation}
		y = softmax( W \times [ pool_p ; pool_h ] + b )
		\end{equation}		
		\\
		Les performances de ces systèmes et autres détails techniques 
		sont décrits dans la partie "résultats expérimentaux" en annexe.\\
		Les schémas de ces systèmes figurent dans la partie "" en annexe.
		
\section{Techniques pour l'interprétabilité}	
	Dans cette section nous allons décrire notre technique pour interpreter une prédiction d'une entrée. Nous voulons donner des explications pour chaque label 
	en donnant les trois mots les plus importants dans la prémisse et les trois mots les plus importants dans l'hypothèse. On veut donc calculer l'importance de chaque mot. 
	L'importance d'un mot correspond à l'importance de sa contribution pour le label $y$.\\
	Notre intuition est la suivante : 
	On veut donner une explication pour le label $y$ et l'entrée $x$ composée de la prémisse et de l'hypothèse. 
	On retire alors un mot et on demande à notre modèle de nous donner les probabilités de chaque label avec cette nouvelle entrée. Si la probabilité 
	du label $y$ a baissé, alors le mot était important : cela veut dire que le mot avait contribué au label $y$. A l'inverse, si elle a augmenté, le mot n'avait donc pas contribué au label $y$. 
	De plus, si la probabilité des autres labels a augmenté, alors le mot a d'autant plus d'importance : en le retirant, l'entrée $x$ a basculé vers un autre label.\\
	\\
	L'importance d'un mot, que nous notons IF pour Impact Factor, est calculé comme suit :
	\begin{equation}
	IF^{y_j}(m_i) = \sum_{j=1}^{|Y|} \log( p(y_j | x \backslash m_i) ) - \log( p(y_j | x) ) 
	\end{equation}
	\begin{equation}
	IF^{y_{ref}}(m_i) = -\log( p(y_{ref} | x \backslash m_i) ) + \log( p(y_{ref} | x) )
	\end{equation}
	\begin{equation}
	IF^{total}(m_i) = IF^{y_{ref}}(m_i) + IF^{y_j}(m_i) 
	\end{equation}\\
	où $y_{ref}$ est le label de référence (le label que l'on veut "expliquer"), $x$ est l'entrée, $m_i$ est le mot que l'on retire de l'entrée $x$, 
	$y_j$ est un label différent de $y_{ref}$, et $|Y|$ est le nombre de label. Les probabilités sont exprimées en log-probabilités pour des soucis de précision et d'optimisation du code. On parle donc plutôt de score.\\
	Cette formule suit notre intuition de base et également celle de Robnik-Sikonja et Kononenko \cite{ref8}. Nous avons rajouté la prise en compte de l'impact sur les scores 
	des autres labels quand on retire le mot $m_i$.\\
	\\
	Un problème persiste tout de même : que se passe-t-il vraiment lorsque l'on retire un mot ? Nous ne pouvons pas simplement le supprimer de l'entrée, car la phrase n'aurait plus de sens 
	et le modèle n'est peut-être pas entraîné à faire face aux bruits. On ne peut pas vraiment le remplacer par un mot générique (le mot "UNK" pour "mot inconnu", par exemple) 
	car un modèle remplacant ce mot par un embedding à 0 et un modèle remplacant ce mot par un embedding particulier auront des réponses très différentes. Or on voudrait que notre méthode soit 
	réalisable par n'importe quel modèle. La solution est donc de remplacer le mot que l'on veut retirer par un autre :
	\begin{equation}
	p(y | x \backslash m_i) =  \sum_{s=1}^{r_i} p(y | x \leftarrow m_i = m_s) 
	\end{equation}\\
	où $r_i$ est le nombre de mot que l'on peut mettre à la place du mot $m_i$ et $m_s$ est le mot que l'on met à la place de $m_i$.\\
	Les mots pouvant remplacer $m_i$ sont situés dans un fichier externe créé par nous-même. On a fixé à 5 le nombre de mots maximum pouvant remplacer $m_i$.
	Plus il y a de mots de remplacements, plus on a une meilleure estimation du IF du mot $m_i$, mais dans ce cas la complexité en temps augmente. Il faut donc trouver un compromis entre 
	l'estimation du IF et le temps que met le programme pour donner une explication.\\
	\\
	Cette méthode permet donc d'avoir une explication contrastée, puisque l'on compare notre entrée de base avec des entrées créées artificiellement en remplacant un mot par un autre. 
	De plus, elle est également sélective puisque l'on sélectionne les K mots, respectivement dans la prémisse puis dans l'hypothèse, ayant le plus contribué au label $y_{ref}$. 
	Enfin, le programme surligne les mots les plus importants pour chaque label, ce qui permet d'avoir une visualisation et donc d'être compris par n'importe qui.
	
	/* METTRE UN SCREEN D'UNE EXPLICATION */
		
\section{Cadre expérimental}
 
	
	\subsection{Blablabla}
		bla bla bla

\section{Remerciements}
 
\bibliographystyle{unsrt} % Le style est mis entre accolades.
\bibliography{biblio}

\renewcommand{\appendixname}{Annexes}
\appendix
\section{Résulats expérimentaux}
Pour la représentation des mots, nous utilisons des words embeddings pré-entrainés de dimension 100 via GloVe.6B.100d.
Pour les mots inconnus, c'est-à-dire les mots qui n'ont pas d'embedding dans GloVe, nous utilisons des embeddings
initialisés au hasard. Tous les embeddings sont mis-à-jour par le réseau.\\
La taille des batches est de 16 et le dropout est à 0,3.\\
Concernant les RNNs utilisés, il n'y a qu'une seule couche et la dimension des états cachés est de 100.\\

\begin{tabular}{|c|p{3cm}|p{3cm}|p{3cm}|p{3cm}|p{3cm}|}
\hline
\small{Système} & \small{Taux de réussite Contradiction} & \small{Taux de réussite Inférence} & \small{Taux de réussite Neutre} & \small{Taux de réussite Dev} & \small{Taux de réussite Test} \\
\hline
1 & 67,33\% & 73,90\% & 68,38\% & 69,89\% & test \\
\hline
2 & 78,55\% & 87,98\% & 74,99\% & 80,57\% & test \\
\hline
3 & 68,73\% & 74,92\% & 69,55\% & 71,09\% & test \\
\hline
\end{tabular}

%pour taux réussite test sys2
%0.800712
	%Contradiction Accuracy = 0.777023
	%Inference Accuracy = 0.861045
	%Neutral Accuracy = 0.761417

\end{document}














